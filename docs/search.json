[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "MA206X Course Syllabus",
    "section": "",
    "text": "Course: MA206X - Probability and Statistics Credits: 3.0 Textbook: Probability and Statistics for Engineering and the Sciences, 9th Ed., by Jay Devore Platform: Cengage WebAssign (for homework)"
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "MA206X Course Syllabus",
    "section": "",
    "text": "Course: MA206X - Probability and Statistics Credits: 3.0 Textbook: Probability and Statistics for Engineering and the Sciences, 9th Ed., by Jay Devore Platform: Cengage WebAssign (for homework)"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "MA206X Course Syllabus",
    "section": "2 Course Description",
    "text": "2 Course Description\nThis course introduces you to the foundational principles of probability and statistics, emphasizing data literacy and inference. It begins with Block I, covering data types, visualization, and basic probability rules including counting and the behavior of random variables. Block II builds on this by exploring discrete and continuous distributions, the Central Limit Theorem, and tools for one-sample inference such as confidence intervals and hypothesis testing for proportions and means. Finally, Block III develops cadets’ ability to analyze relationships between variables through two-sample tests, linear regression, ANOVA, and goodness-of-fit testing. By the end of the course, cadets will be equipped to make sound, data-driven decisions grounded in statistical reasoning."
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "MA206X Course Syllabus",
    "section": "3 Learning Objectives",
    "text": "3 Learning Objectives\nBy completing MA206X, you will be able to:\n\nApply the Six Step Method of statistical investigation\nClassify and visualize data appropriately\nCalculate and interpret descriptive statistics\nApply fundamental probability rules and counting principles\nWork with discrete and continuous probability distributions\nConstruct and interpret confidence intervals\nConduct hypothesis tests for means and proportions\nPerform linear regression and ANOVA\nUse R for statistical analysis and visualization\nCommunicate statistical findings effectively"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "MA206X Course Syllabus",
    "section": "4 Grading",
    "text": "4 Grading\n\n\n\nComponent\nPoints\n\n\n\n\nWebAssign Homework\n150\n\n\nWPR I\n175\n\n\nWPR II\n175\n\n\nExploratory Data Analysis\n25\n\n\nTech Report\n125\n\n\nProject Presentation\n50\n\n\nTerm End Exam (TEE)\n300\n\n\nTotal\n1000"
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "MA206X Course Syllabus",
    "section": "5 Course Policies",
    "text": "5 Course Policies\n\nAll assignments will be due by the due date posted in Canvas with the exception of the WebAssign homeworks, which will be due at the start of class that lesson.\nAll late assignments will be deducted 10% per 24 hour period that has elapsed from the published date time group.\nGenerative AI is encouraged throughout the use of this course. Ensure it is cited IAW the current DAAW and follow the assignment-specific guidance."
  },
  {
    "objectID": "syllabus.html#course-schedule",
    "href": "syllabus.html#course-schedule",
    "title": "MA206X Course Syllabus",
    "section": "6 Course Schedule",
    "text": "6 Course Schedule\n\n6.1 Block I: Data and Randomness (Lessons 1-16)\nFoundational concepts in data types, sampling, exploratory data analysis, probability theory, and discrete/continuous distributions.\nTopics: Data types, sampling methods, bias, measures of location and spread, probability axioms, conditional probability, counting, discrete random variables, binomial/Poisson distributions, continuous random variables, normal/exponential distributions.\nAssessment: WPR I (Lesson 16)\n\n\n6.2 Block II: Inference (Lessons 17-27)\nStatistical inference for single populations and comparisons between two populations.\nTopics: Central Limit Theorem, confidence intervals, hypothesis testing framework, one-sample tests (t-test, z-test), two-sample tests, paired data, Type I/II errors, statistical power.\nAssessment: WPR II (Lesson 27)\n\n\n6.3 Block III: Regression Modeling (Lessons 28-40)\nModeling relationships between variables and analysis of variance.\nTopics: Simple linear regression, multiple regression, categorical predictors, interaction terms, model diagnostics, ANOVA (one-way and two-way), multiple comparisons, project work.\nAssessment: Tech Report (Lesson 36), Project Presentation (Lessons 39-40), TEE"
  },
  {
    "objectID": "syllabus.html#course-project",
    "href": "syllabus.html#course-project",
    "title": "MA206X Course Syllabus",
    "section": "7 Course Project",
    "text": "7 Course Project\nThroughout this course you will be executing a statistical investigation into data hosted inside the Vantage enterprise on actual data from an Army unit. You will:\n\nSelect a dataset from Army Vantage\nFormulate research questions\nConduct exploratory data analysis\nPerform appropriate statistical tests\nBuild regression models\nPresent findings in a technical report and presentation\n\nDeliverables:\n\nEDA Assignment (Lesson 5)\nTechnical Report (Lesson 36)\nPeer Review (Lesson 34)\nIPR Presentation (Lesson 37)\nFinal Presentation (Lessons 39-40)"
  },
  {
    "objectID": "syllabus.html#technology",
    "href": "syllabus.html#technology",
    "title": "MA206X Course Syllabus",
    "section": "8 Technology",
    "text": "8 Technology\n\n8.1 R and RStudio\nR is the primary statistical software for this course. You will use R for:\n\nData manipulation and cleaning\nDescriptive statistics\nData visualization\nStatistical inference\nRegression modeling\n\nResources:\n\nR for Data Science\nRStudio Cheatsheets\nCode Annex - Course-specific R reference\n\n\n\n8.2 WebAssign\nHomework assignments and some assessments will be completed through Cengage WebAssign. Instructions for access are available on Canvas.\n\n\n8.3 Army Vantage\nFor the course project, you will access real Army data through the Vantage platform. Account setup instructions are available on Canvas."
  },
  {
    "objectID": "syllabus.html#additional-resources",
    "href": "syllabus.html#additional-resources",
    "title": "MA206X Course Syllabus",
    "section": "9 Additional Resources",
    "text": "9 Additional Resources\n\nOffice Hours: By appointment\nCourse Website: https://usma-stats.github.io (or as designated)\nCanvas: Assignment submissions, grades, announcements\nMath Resource Center: Drop-in tutoring available"
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "MA206X Course Syllabus",
    "section": "10 Academic Integrity",
    "text": "10 Academic Integrity\nAll work submitted must be your own. Collaboration on homework is encouraged, but submitted work must reflect your own understanding. Unauthorized collaboration on graded assessments will be handled according to the Cadet Honor Code.\nWhen using Generative AI:\n\nCite all AI assistance per DAAW requirements\nUse AI for coding assistance, not for conceptual understanding\nDo not use AI to generate written analysis or interpretation"
  },
  {
    "objectID": "syllabus.html#accommodation-statement",
    "href": "syllabus.html#accommodation-statement",
    "title": "MA206X Course Syllabus",
    "section": "11 Accommodation Statement",
    "text": "11 Accommodation Statement\nIf you require accommodations for a documented disability, please contact the Office of the Dean and provide documentation as early as possible in the semester."
  },
  {
    "objectID": "syllabus.html#detailed-lesson-schedule",
    "href": "syllabus.html#detailed-lesson-schedule",
    "title": "MA206X Course Syllabus",
    "section": "12 Detailed Lesson Schedule",
    "text": "12 Detailed Lesson Schedule\n\n12.1 Block I: Data and Randomness\n\n\n\nLSN\nTopic\nReading\n\n\n\n\n1\nTypes of Data, Sampling, & Study Design\n1.1\n\n\n2\nSampling & Study Design\n1.2\n\n\n3\nMeasures of Location\n1.3\n\n\n4\nMeasures of Variability\n1.4\n\n\n5\nExploratory Data Analysis Lab\n-\n\n\n6\nProbability Basics\n2.1, 2.2\n\n\n7\nConditional Probability\n2.4\n\n\n8\nIndependence\n2.3, 2.5\n\n\n9\nDiscrete Random Variables\n3.1-3.3\n\n\n10\nBinomial Distribution\n3.4\n\n\n11\nPoisson Distribution\n3.6\n\n\n12\nContinuous Random Variables\n4.1, 4.2\n\n\n13\nNormal Distribution\n4.3\n\n\n14\nExponential Distribution\n4.4\n\n\n15\nReview\n-\n\n\n16\nWPR I\n-\n\n\n\n\n\n12.2 Block II: Inference\n\n\n\nLSN\nTopic\nReading\n\n\n\n\n17\nCentral Limit Theorem\n5.3, 5.4\n\n\n18\nConfidence Intervals I\n7.1, 7.2\n\n\n19\nConfidence Intervals II\n7.3\n\n\n20\nIntro to Hypothesis Testing\n8.1, 8.2\n\n\n21\nOne Sample t-Test\n8.3\n\n\n22\nOne Proportion Z-Test\n8.4\n\n\n23\nTwo Sample t-Test\n9.2\n\n\n24\nPaired t-Test\n9.3\n\n\n25\nTwo Population Proportions\n9.4\n\n\n26\nReview\n-\n\n\n27\nWPR II\n-\n\n\n\n\n\n12.3 Block III: Regression Modeling\n\n\n\nLSN\nTopic\nReading\n\n\n\n\n28\nSimple Linear Regression I\n12.1, 12.2\n\n\n29\nCourse Drop (Plebe Parent Weekend)\n-\n\n\n30\nSimple Linear Regression II\n12.3, 12.5\n\n\n31\nMultiple Linear Regression I\nSupplement\n\n\n32\nMultiple Linear Regression II\nSupplement\n\n\n33\nMultiple Linear Regression III\nSupplement\n\n\n34\nProject Peer Review\n-\n\n\n35\nCourse Lecture Drop\n-\n\n\n36\nANOVA I\n10.1\n\n\n37\nProject IPR\n-\n\n\n38\nANOVA II\n10.2\n\n\n39-40\nProject Presentations\n-\n\n\n\n\nFor the most current information, assignments, and announcements, refer to Canvas."
  },
  {
    "objectID": "lesson_05.html",
    "href": "lesson_05.html",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "",
    "text": "By the end of this lesson, you will be able to:\n\nExecute EDA using appropriate graphs and summaries\nJustify choices of displays for variable types\nCommunicate findings with clear, concise annotations\n\nReading: Review Devore 1.1-1.4"
  },
  {
    "objectID": "lesson_05.html#introduction",
    "href": "lesson_05.html#introduction",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "1 Introduction",
    "text": "1 Introduction\nExploratory Data Analysis (EDA) is the critical first step in any statistical investigation. Before running formal tests or building models, we must:\n\nUnderstand the structure of our data\nIdentify patterns, trends, and anomalies\nCheck for outliers and data quality issues\nGenerate hypotheses for further investigation\n\nThis lesson brings together everything we’ve learned in Lessons 1-4 through hands-on practice with real datasets."
  },
  {
    "objectID": "lesson_05.html#the-eda-workflow",
    "href": "lesson_05.html#the-eda-workflow",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "2 The EDA Workflow",
    "text": "2 The EDA Workflow\n\n\n\n\n\n\nStandard EDA Process\n\n\n\n\nLoad and inspect the data structure\nClean the data (handle missing values, fix types)\nSummarize with descriptive statistics\nVisualize distributions and relationships\nIdentify patterns, outliers, and anomalies\nDocument findings and insights"
  },
  {
    "objectID": "lesson_05.html#case-study-1-military-physical-fitness-data",
    "href": "lesson_05.html#case-study-1-military-physical-fitness-data",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "3 Case Study 1: Military Physical Fitness Data",
    "text": "3 Case Study 1: Military Physical Fitness Data\nLet’s analyze simulated ACFT data for 100 soldiers.\n\n3.1 Step 1: Load and Inspect\n\n\nCode\n# Create simulated ACFT dataset\nacft_data &lt;- tibble(\n  soldier_id = 1:100,\n  age = sample(18:45, 100, replace = TRUE),\n  gender = sample(c(\"Male\", \"Female\"), 100, replace = TRUE, prob = c(0.85, 0.15)),\n  deadlift = round(rnorm(100, mean = 280, sd = 40)),\n  spt = round(rnorm(100, mean = 9.5, sd = 1.5), 1),\n  hrpu = round(rnorm(100, mean = 55, sd = 12)),\n  sdc = round(rnorm(100, mean = 2.8, sd = 0.4), 1),\n  plank = round(rnorm(100, mean = 3.2, sd = 0.6), 1),\n  run = round(rnorm(100, mean = 14.5, sd = 2.0), 1)\n) %&gt;%\n  mutate(\n    # Calculate total score (simplified scoring)\n    total_score = (deadlift/3) + (15-spt)*20 + hrpu*2 + (4-sdc)*50 + plank*25 + (20-run)*10\n  ) %&gt;%\n  mutate(total_score = pmax(360, pmin(600, round(total_score))))\n\n# View first few rows\nhead(acft_data, 10)\n\n\n# A tibble: 10 × 10\n   soldier_id   age gender deadlift   spt  hrpu   sdc plank   run total_score\n        &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1          1    32 Male        304   8.6    57   3.3   2.7   8.7         559\n 2          2    19 Male        234  10.2    63   2.4   3.1  16.1         496\n 3          3    38 Male        310   7.7    48   3.1   3.2  12.7         543\n 4          4    35 Female      283   9.7    50   2.7   3.3  11.4         534\n 5          5    37 Male        304   6.3    55   2.9   2.9  17.1         542\n 6          6    43 Male        347   9.8    60   3.1   4    16.7         518\n 7          7    18 Male        289   8.5    57   2.4   3.9  13.8         580\n 8          8    25 Male        228  11.4    42   2.3   3.7  10.9         500\n 9          9    33 Male        335  10.9    61   2.3   2.1  11.8         535\n10         10    21 Male        225   8.7    71   2.9   4.4  14.6         562\n\n\n\n\n3.2 Step 2: Check Data Structure\n\n\nCode\n# Data dimensions\ndim(acft_data)\n\n\n[1] 100  10\n\n\nCode\n# Variable types\nstr(acft_data)\n\n\ntibble [100 × 10] (S3: tbl_df/tbl/data.frame)\n $ soldier_id : int [1:100] 1 2 3 4 5 6 7 8 9 10 ...\n $ age        : int [1:100] 32 19 38 35 37 43 18 25 33 21 ...\n $ gender     : chr [1:100] \"Male\" \"Male\" \"Male\" \"Female\" ...\n $ deadlift   : num [1:100] 304 234 310 283 304 347 289 228 335 225 ...\n $ spt        : num [1:100] 8.6 10.2 7.7 9.7 6.3 9.8 8.5 11.4 10.9 8.7 ...\n $ hrpu       : num [1:100] 57 63 48 50 55 60 57 42 61 71 ...\n $ sdc        : num [1:100] 3.3 2.4 3.1 2.7 2.9 3.1 2.4 2.3 2.3 2.9 ...\n $ plank      : num [1:100] 2.7 3.1 3.2 3.3 2.9 4 3.9 3.7 2.1 4.4 ...\n $ run        : num [1:100] 8.7 16.1 12.7 11.4 17.1 16.7 13.8 10.9 11.8 14.6 ...\n $ total_score: num [1:100] 559 496 543 534 542 518 580 500 535 562 ...\n\n\nCode\n# Summary statistics\nsummary(acft_data)\n\n\n   soldier_id          age           gender             deadlift    \n Min.   :  1.00   Min.   :18.00   Length:100         Min.   :176.0  \n 1st Qu.: 25.75   1st Qu.:25.00   Class :character   1st Qu.:252.5  \n Median : 50.50   Median :31.50   Mode  :character   Median :284.5  \n Mean   : 50.50   Mean   :31.33                      Mean   :282.6  \n 3rd Qu.: 75.25   3rd Qu.:38.00                      3rd Qu.:314.0  \n Max.   :100.00   Max.   :45.00                      Max.   :376.0  \n      spt              hrpu            sdc            plank      \n Min.   : 5.300   Min.   :26.00   Min.   :1.700   Min.   :1.400  \n 1st Qu.: 8.175   1st Qu.:45.00   1st Qu.:2.600   1st Qu.:2.700  \n Median : 9.250   Median :54.50   Median :2.800   Median :3.100  \n Mean   : 9.244   Mean   :53.95   Mean   :2.819   Mean   :3.123  \n 3rd Qu.:10.200   3rd Qu.:64.00   3rd Qu.:3.025   3rd Qu.:3.525  \n Max.   :12.800   Max.   :80.00   Max.   :3.700   Max.   :4.900  \n      run         total_score   \n Min.   : 8.70   Min.   :404.0  \n 1st Qu.:12.80   1st Qu.:474.2  \n Median :14.30   Median :502.0  \n Mean   :14.26   Mean   :509.9  \n 3rd Qu.:15.50   3rd Qu.:548.5  \n Max.   :20.70   Max.   :600.0  \n\n\n\n\n3.3 Step 3: Univariate Analysis - Categorical Variables\n\n\nCode\n# Frequency table for gender\ngender_table &lt;- acft_data %&gt;%\n  count(gender) %&gt;%\n  mutate(percentage = round(n/sum(n)*100, 1))\n\ngender_table %&gt;%\n  kable(col.names = c(\"Gender\", \"Count\", \"Percentage\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nGender\nCount\nPercentage\n\n\n\n\nFemale\n9\n9\n\n\nMale\n91\n91\n\n\n\n\n\nVisualization:\n\n\nCode\nggplot(acft_data, aes(x = gender, fill = gender)) +\n  geom_bar(alpha = 0.7) +\n  geom_text(stat = \"count\", aes(label = after_stat(count)), vjust = -0.5) +\n  scale_fill_manual(values = c(\"coral\", \"steelblue\")) +\n  labs(title = \"Distribution of Soldiers by Gender\",\n       x = \"Gender\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n3.4 Step 4: Univariate Analysis - Quantitative Variables\nLet’s analyze the distribution of total ACFT scores:\n\n\nCode\n# Summary statistics\nscore_summary &lt;- acft_data %&gt;%\n  summarise(\n    n = n(),\n    Mean = round(mean(total_score), 1),\n    SD = round(sd(total_score), 1),\n    Min = min(total_score),\n    Q1 = quantile(total_score, 0.25),\n    Median = median(total_score),\n    Q3 = quantile(total_score, 0.75),\n    Max = max(total_score),\n    IQR = IQR(total_score),\n    Range = max(total_score) - min(total_score)\n  )\n\nscore_summary %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nn\nMean\nSD\nMin\nQ1\nMedian\nQ3\nMax\nIQR\nRange\n\n\n\n\n100\n509.9\n52.9\n404\n474.25\n502\n548.5\n600\n74.25\n196\n\n\n\n\n\nMultiple Visualizations:\n\n\nCode\n# Histogram\np1 &lt;- ggplot(acft_data, aes(x = total_score)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  geom_vline(xintercept = mean(acft_data$total_score),\n             color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median(acft_data$total_score),\n             color = \"darkgreen\", linetype = \"dotted\", linewidth = 1) +\n  labs(title = \"Histogram of Total ACFT Scores\",\n       subtitle = \"Red = Mean, Green = Median\",\n       x = \"Total Score\", y = \"Frequency\") +\n  theme_minimal()\n\n# Boxplot\np2 &lt;- ggplot(acft_data, aes(y = total_score)) +\n  geom_boxplot(fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Boxplot of Total ACFT Scores\",\n       y = \"Total Score\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n# Density plot\np3 &lt;- ggplot(acft_data, aes(x = total_score)) +\n  geom_density(fill = \"steelblue\", alpha = 0.5) +\n  labs(title = \"Density Plot of Total ACFT Scores\",\n       x = \"Total Score\", y = \"Density\") +\n  theme_minimal()\n\n# QQ plot to check normality\np4 &lt;- ggplot(acft_data, aes(sample = total_score)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot: Checking Normality\",\n       x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal()\n\n# Combine plots\n(p1 | p2) / (p3 | p4)\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nDistribution appears roughly symmetric (mean ≈ median)\nNo extreme outliers visible in boxplot\nQ-Q plot shows slight deviations from normality at tails\nMost scores cluster between 450-500\n\n\n\n3.5 Step 5: Bivariate Analysis - Two Categorical Variables\n\n\nCode\n# Cross-tabulation\ngender_age_table &lt;- acft_data %&gt;%\n  mutate(age_group = cut(age, breaks = c(17, 25, 35, 50),\n                         labels = c(\"18-25\", \"26-35\", \"36+\"))) %&gt;%\n  count(gender, age_group) %&gt;%\n  pivot_wider(names_from = age_group, values_from = n, values_fill = 0)\n\ngender_age_table %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\ngender\n18-25\n26-35\n36+\n\n\n\n\nFemale\n3\n3\n3\n\n\nMale\n28\n33\n30\n\n\n\n\n\nVisualization:\n\n\nCode\nacft_data %&gt;%\n  mutate(age_group = cut(age, breaks = c(17, 25, 35, 50),\n                         labels = c(\"18-25\", \"26-35\", \"36+\"))) %&gt;%\n  ggplot(aes(x = age_group, fill = gender)) +\n  geom_bar(position = \"dodge\", alpha = 0.7) +\n  scale_fill_manual(values = c(\"coral\", \"steelblue\")) +\n  labs(title = \"Distribution by Age Group and Gender\",\n       x = \"Age Group\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.6 Step 6: Bivariate Analysis - Categorical vs. Quantitative\nHow do total scores differ by gender?\n\n\nCode\n# Summary by gender\ngender_scores &lt;- acft_data %&gt;%\n  group_by(gender) %&gt;%\n  summarise(\n    n = n(),\n    Mean = round(mean(total_score), 1),\n    SD = round(sd(total_score), 1),\n    Median = median(total_score),\n    IQR = IQR(total_score)\n  )\n\ngender_scores %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\ngender\nn\nMean\nSD\nMedian\nIQR\n\n\n\n\nFemale\n9\n529\n61.7\n534\n112.0\n\n\nMale\n91\n508\n52.0\n500\n72.5\n\n\n\n\n\nVisualizations:\n\n\nCode\n# Side-by-side boxplots\np1 &lt;- ggplot(acft_data, aes(x = gender, y = total_score, fill = gender)) +\n  geom_boxplot(alpha = 0.7) +\n  scale_fill_manual(values = c(\"coral\", \"steelblue\")) +\n  labs(title = \"ACFT Scores by Gender (Boxplot)\",\n       x = \"Gender\", y = \"Total Score\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Overlapping density plots\np2 &lt;- ggplot(acft_data, aes(x = total_score, fill = gender)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"coral\", \"steelblue\")) +\n  labs(title = \"ACFT Scores by Gender (Density)\",\n       x = \"Total Score\", y = \"Density\") +\n  theme_minimal()\n\np1 | p2\n\n\n\n\n\n\n\n\n\n\n\n3.7 Step 7: Bivariate Analysis - Two Quantitative Variables\nIs there a relationship between age and total ACFT score?\n\n\nCode\n# Scatterplot\nggplot(acft_data, aes(x = age, y = total_score)) +\n  geom_point(alpha = 0.6, color = \"steelblue\", size = 2) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(title = \"Relationship Between Age and ACFT Score\",\n       subtitle = paste(\"Correlation:\", round(cor(acft_data$age, acft_data$total_score), 3)),\n       x = \"Age (years)\", y = \"Total ACFT Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate correlation\ncorrelation &lt;- cor(acft_data$age, acft_data$total_score)\ncat(\"Correlation coefficient:\", round(correlation, 3), \"\\n\")\n\n\nCorrelation coefficient: -0.009 \n\n\nInterpretation:\n\nWeak negative correlation between age and score\nAs age increases, scores tend to decrease slightly\nRelationship is not very strong (r ≈ -0.009)\n\n\n\n3.8 Step 8: Multivariate Analysis\nLet’s examine all event scores together:\n\n\nCode\n# Reshape data for faceted plots\nacft_long &lt;- acft_data %&gt;%\n  select(soldier_id, deadlift, spt, hrpu, sdc, plank, run) %&gt;%\n  pivot_longer(cols = -soldier_id, names_to = \"event\", values_to = \"score\")\n\n# Faceted histograms\nggplot(acft_long, aes(x = score, fill = event)) +\n  geom_histogram(bins = 15, alpha = 0.7, show.legend = FALSE) +\n  facet_wrap(~event, scales = \"free\") +\n  labs(title = \"Distribution of Scores Across All ACFT Events\",\n       x = \"Score\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCorrelation matrix:\n\n\nCode\n# Select only numeric event columns\ncor_matrix &lt;- acft_data %&gt;%\n  select(deadlift, spt, hrpu, sdc, plank, run, total_score) %&gt;%\n  cor() %&gt;%\n  round(2)\n\ncor_matrix %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), font_size = 11)\n\n\n\n\n\n\ndeadlift\nspt\nhrpu\nsdc\nplank\nrun\ntotal_score\n\n\n\n\ndeadlift\n1.00\n-0.12\n0.00\n-0.11\n0.01\n-0.05\n0.37\n\n\nspt\n-0.12\n1.00\n0.00\n0.06\n-0.02\n0.09\n-0.61\n\n\nhrpu\n0.00\n0.00\n1.00\n-0.09\n0.05\n0.11\n0.46\n\n\nsdc\n-0.11\n0.06\n-0.09\n1.00\n-0.07\n0.00\n-0.46\n\n\nplank\n0.01\n-0.02\n0.05\n-0.07\n1.00\n0.05\n0.31\n\n\nrun\n-0.05\n0.09\n0.11\n0.00\n0.05\n1.00\n-0.35\n\n\ntotal_score\n0.37\n-0.61\n0.46\n-0.46\n0.31\n-0.35\n1.00\n\n\n\n\n\n\n\n3.9 Step 9: Identify Outliers\n\n\nCode\n# Use IQR method to detect outliers in total score\nQ1 &lt;- quantile(acft_data$total_score, 0.25)\nQ3 &lt;- quantile(acft_data$total_score, 0.75)\nIQR_val &lt;- Q3 - Q1\n\nlower_fence &lt;- Q1 - 1.5 * IQR_val\nupper_fence &lt;- Q3 + 1.5 * IQR_val\n\noutliers &lt;- acft_data %&gt;%\n  filter(total_score &lt; lower_fence | total_score &gt; upper_fence) %&gt;%\n  select(soldier_id, age, gender, total_score)\n\ncat(\"Outlier detection thresholds:\\n\")\n\n\nOutlier detection thresholds:\n\n\nCode\ncat(\"Lower fence:\", lower_fence, \"\\n\")\n\n\nLower fence: 362.875 \n\n\nCode\ncat(\"Upper fence:\", upper_fence, \"\\n\\n\")\n\n\nUpper fence: 659.875 \n\n\nCode\nif(nrow(outliers) &gt; 0) {\n  cat(\"Outliers detected:\\n\")\n  print(outliers)\n} else {\n  cat(\"No outliers detected.\\n\")\n}\n\n\nNo outliers detected."
  },
  {
    "objectID": "lesson_05.html#case-study-2-diamonds-dataset",
    "href": "lesson_05.html#case-study-2-diamonds-dataset",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "4 Case Study 2: Diamonds Dataset",
    "text": "4 Case Study 2: Diamonds Dataset\nLet’s practice EDA on a different dataset - the classic diamonds dataset from ggplot2.\n\n\nCode\n# Load diamonds data\ndata(diamonds)\n\n# Take a random sample for easier computation\ndiamonds_sample &lt;- diamonds %&gt;% sample_n(1000)\n\n# Quick overview\nglimpse(diamonds_sample)\n\n\nRows: 1,000\nColumns: 10\n$ carat   &lt;dbl&gt; 0.33, 0.90, 1.28, 1.02, 0.46, 1.00, 0.30, 0.40, 1.57, 0.70, 0.…\n$ cut     &lt;ord&gt; Ideal, Good, Very Good, Very Good, Ideal, Premium, Ideal, Idea…\n$ color   &lt;ord&gt; D, G, F, H, G, H, G, E, I, E, E, F, E, G, F, G, G, I, E, I, J,…\n$ clarity &lt;ord&gt; VS2, SI1, SI2, SI1, IF, VS2, IF, VS2, SI2, SI1, VVS1, SI2, SI1…\n$ depth   &lt;dbl&gt; 61.8, 63.9, 62.5, 61.9, 61.6, 61.9, 62.2, 62.3, 62.8, 60.2, 62…\n$ table   &lt;dbl&gt; 54.0, 60.0, 55.0, 61.0, 54.0, 61.0, 56.0, 56.0, 57.0, 57.0, 59…\n$ price   &lt;int&gt; 681, 3574, 5223, 4476, 1558, 4536, 850, 1025, 7385, 2575, 740,…\n$ x       &lt;dbl&gt; 4.43, 6.07, 6.91, 6.34, 4.97, 6.37, 4.26, 4.78, 7.39, 5.78, 4.…\n$ y       &lt;dbl&gt; 4.47, 6.10, 7.00, 6.42, 5.00, 6.23, 4.29, 4.72, 7.36, 5.82, 3.…\n$ z       &lt;dbl&gt; 2.75, 3.89, 4.35, 3.95, 3.07, 3.90, 2.66, 2.96, 4.63, 3.49, 2.…\n\n\n\n4.1 Task 1: Explore Price Distribution\n\n\nCode\n# Summary statistics\ndiamonds_sample %&gt;%\n  summarise(\n    Mean = round(mean(price), 2),\n    Median = median(price),\n    SD = round(sd(price), 2),\n    IQR = IQR(price),\n    Min = min(price),\n    Max = max(price)\n  ) %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nMean\nMedian\nSD\nIQR\nMin\nMax\n\n\n\n\n3904.75\n2426\n3966.97\n4106.25\n374\n18653\n\n\n\n\n\nCode\n# Histogram with mean and median\nggplot(diamonds_sample, aes(x = price)) +\n  geom_histogram(bins = 30, fill = \"darkgreen\", color = \"white\", alpha = 0.7) +\n  geom_vline(xintercept = mean(diamonds_sample$price),\n             color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median(diamonds_sample$price),\n             color = \"blue\", linetype = \"dotted\", linewidth = 1) +\n  labs(title = \"Distribution of Diamond Prices\",\n       subtitle = \"Red = Mean | Blue = Median\",\n       x = \"Price (USD)\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nObservation: Right-skewed distribution (mean &gt; median), indicating high-priced diamonds pull the average up.\n\n\n4.2 Task 2: Compare Prices Across Cut Quality\n\n\nCode\n# Summary by cut\ncut_summary &lt;- diamonds_sample %&gt;%\n  group_by(cut) %&gt;%\n  summarise(\n    n = n(),\n    Mean_Price = round(mean(price), 2),\n    Median_Price = median(price),\n    SD = round(sd(price), 2)\n  ) %&gt;%\n  arrange(desc(Mean_Price))\n\ncut_summary %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\ncut\nn\nMean_Price\nMedian_Price\nSD\n\n\n\n\nPremium\n268\n4714.88\n3288.5\n4357.73\n\n\nFair\n27\n4462.93\n3688.0\n3530.46\n\n\nVery Good\n226\n3771.23\n2682.5\n3625.31\n\n\nGood\n95\n3694.39\n2777.0\n3292.45\n\n\nIdeal\n384\n3430.72\n1684.5\n3981.63\n\n\n\n\n\nCode\n# Boxplot comparison\nggplot(diamonds_sample, aes(x = cut, y = price, fill = cut)) +\n  geom_boxplot(alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Diamond Prices by Cut Quality\",\n       x = \"Cut Quality\", y = \"Price (USD)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n4.3 Task 3: Relationship Between Carat and Price\n\n\nCode\n# Scatterplot\nggplot(diamonds_sample, aes(x = carat, y = price, color = cut)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\") +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Relationship Between Carat Weight and Price\",\n       subtitle = paste(\"Overall Correlation:\", round(cor(diamonds_sample$carat, diamonds_sample$price), 3)),\n       x = \"Carat Weight\", y = \"Price (USD)\",\n       color = \"Cut Quality\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Correlation\ncor(diamonds_sample$carat, diamonds_sample$price)\n\n\n[1] 0.9239175\n\n\nFinding: Strong positive correlation (r ≈ 0.92) between carat and price!"
  },
  {
    "objectID": "lesson_05.html#your-turn-practice-eda",
    "href": "lesson_05.html#your-turn-practice-eda",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "5 Your Turn: Practice EDA",
    "text": "5 Your Turn: Practice EDA\n\n5.1 Practice Exercise\nUsing the mtcars dataset, perform a complete EDA:\n\n\nCode\n# Load the dataset\ndata(mtcars)\n\n# Your analysis here:\n# 1. Inspect structure and summary\n# 2. Analyze the distribution of mpg (miles per gallon)\n# 3. Compare mpg across different numbers of cylinders (cyl)\n# 4. Examine relationship between weight (wt) and mpg\n# 5. Identify any outliers\n# 6. Create appropriate visualizations\n\n\nGuiding Questions:\n\nWhat is the typical fuel efficiency (mpg)?\nHow does mpg vary by number of cylinders?\nIs there a relationship between car weight and fuel efficiency?\nAre there any unusual cars (outliers)?"
  },
  {
    "objectID": "lesson_05.html#eda-best-practices",
    "href": "lesson_05.html#eda-best-practices",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "6 EDA Best Practices",
    "text": "6 EDA Best Practices\n\n\n\n\n\n\nGuidelines for Effective EDA\n\n\n\n\nStart simple: Basic summaries and plots first\nChoose appropriate visualizations:\n\nCategorical: Bar charts, pie charts\nQuantitative: Histograms, boxplots, density plots\nTwo quantitative: Scatterplots\nCategorical + Quantitative: Side-by-side boxplots\n\nLook for patterns:\n\nTrends, clusters, outliers\nSkewness and symmetry\nUnusual observations\n\nCompare measures:\n\nMean vs. median (skewness indicator)\nRange vs. IQR (outlier sensitivity)\n\nDocument everything:\n\nClear titles and labels\nInterpretation of findings\nQuestions for further investigation\n\nBe skeptical:\n\nCheck data quality\nVerify unusual patterns\nConsider alternative explanations"
  },
  {
    "objectID": "lesson_05.html#common-pitfalls-to-avoid",
    "href": "lesson_05.html#common-pitfalls-to-avoid",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "7 Common Pitfalls to Avoid",
    "text": "7 Common Pitfalls to Avoid\n\n\n\n\n\n\nEDA Mistakes\n\n\n\n\nJumping to conclusions without exploring data first\nUsing inappropriate visualizations for data type\nIgnoring outliers or blindly removing them\nNot checking assumptions (e.g., normality)\nForgetting to label axes or provide context\nOverinterpreting correlation as causation\nNot considering practical significance vs. statistical patterns"
  },
  {
    "objectID": "lesson_05.html#eda-checklist",
    "href": "lesson_05.html#eda-checklist",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "8 EDA Checklist",
    "text": "8 EDA Checklist\nUse this checklist for any dataset:\n\n\n\n\n\nStep\nTask\nCompleted\n\n\n\n\n1\nLoad data and check dimensions\n☐\n\n\n2\nInspect variable types and structure\n☐\n\n\n3\nCheck for missing values\n☐\n\n\n4\nCalculate summary statistics (mean, median, SD, quartiles)\n☐\n\n\n5\nCreate histograms/boxplots for key quantitative variables\n☐\n\n\n6\nCreate bar charts for categorical variables\n☐\n\n\n7\nIdentify and investigate outliers\n☐\n\n\n8\nExplore relationships between variables (scatterplots, correlations)\n☐\n\n\n9\nCheck distribution shapes (symmetric, skewed, normal?)\n☐\n\n\n10\nDocument findings and generate hypotheses\n☐"
  },
  {
    "objectID": "lesson_05.html#summary",
    "href": "lesson_05.html#summary",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "9 Summary",
    "text": "9 Summary\nIn this lab, we practiced:\n\nSystematic data exploration following a structured workflow\nAppropriate visualization choices based on variable types:\n\nCategorical: Bar charts\nQuantitative: Histograms, boxplots, density plots\nRelationships: Scatterplots, side-by-side boxplots\n\nComprehensive summaries using:\n\nMeasures of location (mean, median)\nMeasures of spread (SD, IQR, range)\nFive-number summary\n\nPattern recognition:\n\nDistribution shapes\nOutliers\nRelationships and correlations\n\nClear communication of findings through:\n\nWell-labeled visualizations\nInterpretive text\nOrganized presentation\n\n\nLooking Ahead: In Block I Lessons 6-14, we’ll move from describing data (EDA) to probability theory and random variables, which provide the theoretical foundation for statistical inference."
  },
  {
    "objectID": "lesson_05.html#additional-resources",
    "href": "lesson_05.html#additional-resources",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "10 Additional Resources",
    "text": "10 Additional Resources\n\nR for Data Science - EDA Chapter\nggplot2 Documentation\nPractice your EDA skills with the WebAssign EDA assignment\nExplore additional datasets:\n\niris - Fisher’s iris data\nmpg - Fuel economy data\nstarwars - Star Wars character data"
  },
  {
    "objectID": "lesson_05.html#take-home-challenge",
    "href": "lesson_05.html#take-home-challenge",
    "title": "Lesson 5: Exploratory Data Analysis Lab",
    "section": "11 Take-Home Challenge",
    "text": "11 Take-Home Challenge\nChoose one of the following datasets and perform a complete EDA. Prepare a 1-page summary with:\n\n2-3 key visualizations\nSummary statistics table\n3-5 bullet points of key findings\n\nOptions:\n\nairquality - Daily air quality measurements in New York\nchickwts - Chicken weight by feed type\nYour own dataset from Army Vantage (for course project!)\n\n\n\nCode\n# Example: Load a dataset\ndata(airquality)\n\n# Start your EDA here!"
  },
  {
    "objectID": "lesson_03.html",
    "href": "lesson_03.html",
    "title": "Lesson 3: Measures of Location",
    "section": "",
    "text": "By the end of this lesson, you will be able to:\n\nCalculate and interpret mean, median, and mode\nDetermine percentiles and quartiles\nCompare measures of center for different distributions\n\nReading: Devore 1.3"
  },
  {
    "objectID": "lesson_03.html#introduction",
    "href": "lesson_03.html#introduction",
    "title": "Lesson 3: Measures of Location",
    "section": "1 Introduction",
    "text": "1 Introduction\nAfter collecting data, our first task is to summarize it. With potentially hundreds or thousands of observations, we need ways to describe the “center” or “typical value” of our data.\nMeasures of location (also called measures of central tendency) answer the question: “What is a representative or typical value in this dataset?”\nWe’ll explore three primary measures:\n\nMean (average)\nMedian (middle value)\nMode (most frequent value)"
  },
  {
    "objectID": "lesson_03.html#the-sample-mean",
    "href": "lesson_03.html#the-sample-mean",
    "title": "Lesson 3: Measures of Location",
    "section": "2 The Sample Mean",
    "text": "2 The Sample Mean\n\n\n\n\n\n\nDefinition: Sample Mean\n\n\n\nThe sample mean \\(\\bar{x}\\) (pronounced “x-bar”) is the arithmetic average of all observations:\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\]\nwhere \\(n\\) is the sample size.\n\n\n\n2.1 Calculating the Mean\n\n\n2.2 Example 3.1: Mean APFT Score\nFive soldiers complete the APFT with the following scores:\n\\[285, 270, 295, 260, 290\\]\nCalculate the mean score:\n\\[\\bar{x} = \\frac{285 + 270 + 295 + 260 + 290}{5} = \\frac{1400}{5} = 280\\]\nInterpretation: The average APFT score is 280 points.\n\n\n2.3 Computing the Mean in R\n\n\nCode\n# APFT scores\napft_scores &lt;- c(285, 270, 295, 260, 290)\n\n# Calculate mean\nmean_score &lt;- mean(apft_scores)\nmean_score\n\n\n[1] 280\n\n\n\n\n2.4 Properties of the Mean\n\nUses all data: Every observation contributes to the mean\nSensitive to outliers: Extreme values can heavily influence the mean\nBalance point: The mean is the value where positive and negative deviations balance out: \\[\\sum_{i=1}^n (x_i - \\bar{x}) = 0\\]\n\n\n\n2.5 Example 3.2: Effect of Outliers on Mean\nConsider two datasets of ruck march times (in minutes):\nDataset A (no outliers):\n\n\nCode\nruck_A &lt;- c(95, 98, 100, 102, 105)\nmean(ruck_A)\n\n\n[1] 100\n\n\nDataset B (with outlier):\n\n\nCode\nruck_B &lt;- c(95, 98, 100, 102, 150)  # One soldier took much longer\nmean(ruck_B)\n\n\n[1] 109\n\n\nThe single outlier (150) increased the mean from 100 to 109 minutes!"
  },
  {
    "objectID": "lesson_03.html#the-median",
    "href": "lesson_03.html#the-median",
    "title": "Lesson 3: Measures of Location",
    "section": "3 The Median",
    "text": "3 The Median\n\n\n\n\n\n\nDefinition: Median\n\n\n\nThe median is the middle value when observations are arranged in order.\nCalculation:\n\nSort data from smallest to largest\nIf \\(n\\) is odd: median = middle value\nIf \\(n\\) is even: median = average of two middle values\n\nPosition: The median is at position \\(\\frac{n+1}{2}\\) (may be between two values)\n\n\n\n3.1 Calculating the Median\n\n\n3.2 Example 3.3: Median with Odd Sample Size\nRuck march times (minutes) for 5 soldiers:\n\\[105, 95, 102, 98, 100\\]\nStep 1: Sort the data: \\[95, 98, 100, 102, 105\\]\nStep 2: Find middle value (position 3): \\[\\text{Median} = 100\\]\n\n\nCode\nruck_odd &lt;- c(105, 95, 102, 98, 100)\nmedian(ruck_odd)\n\n\n[1] 100\n\n\n\n\n3.3 Example 3.4: Median with Even Sample Size\nRuck march times (minutes) for 6 soldiers:\n\\[95, 98, 100, 102, 105, 110\\]\nStep 1: Data already sorted\nStep 2: Average the two middle values (positions 3 and 4): \\[\\text{Median} = \\frac{100 + 102}{2} = 101\\]\n\n\nCode\nruck_even &lt;- c(95, 98, 100, 102, 105, 110)\nmedian(ruck_even)\n\n\n[1] 101\n\n\n\n\n3.4 The Median is Resistant to Outliers\nRevisiting our outlier example:\n\n\nCode\n# Dataset A\nruck_A &lt;- c(95, 98, 100, 102, 105)\ncat(\"Mean:\", mean(ruck_A), \"  Median:\", median(ruck_A), \"\\n\")\n\n\nMean: 100   Median: 100 \n\n\nCode\n# Dataset B (with outlier)\nruck_B &lt;- c(95, 98, 100, 102, 150)\ncat(\"Mean:\", mean(ruck_B), \"  Median:\", median(ruck_B), \"\\n\")\n\n\nMean: 109   Median: 100 \n\n\nThe median (100) didn’t change! This is why median is called a resistant measure."
  },
  {
    "objectID": "lesson_03.html#the-mode",
    "href": "lesson_03.html#the-mode",
    "title": "Lesson 3: Measures of Location",
    "section": "4 The Mode",
    "text": "4 The Mode\n\n\n\n\n\n\nDefinition: Mode\n\n\n\nThe mode is the most frequently occurring value in a dataset.\nNote: A dataset can have:\n\nNo mode (all values occur once)\nOne mode (unimodal)\nMultiple modes (bimodal, multimodal)\n\n\n\n\n4.1 Example 3.5: Finding the Mode\nDataset 1: Number of pull-ups in a platoon\n\n\nCode\npullups &lt;- c(12, 15, 15, 18, 15, 20, 18, 15, 22, 15)\n\n# Find mode (manually - R doesn't have a built-in mode function)\ntable(pullups)\n\n\npullups\n12 15 18 20 22 \n 1  5  2  1  1 \n\n\nMode = 15 (appears 5 times)\nDataset 2: ACFT scores (rounded to nearest 10)\n\n\nCode\nacft_rounded &lt;- c(450, 460, 470, 470, 480, 490, 490, 500)\ntable(acft_rounded)\n\n\nacft_rounded\n450 460 470 480 490 500 \n  1   1   2   1   2   1 \n\n\nBimodal: Modes are 470 and 490 (each appears twice)\n\n\n4.2 When is Mode Useful?\n\nCategorical data: Mode is the only measure of center that makes sense\nExample: Most common branch, most popular meal choice\nDiscrete data with repeats: When values naturally cluster\nExample: Most common number of vehicles per household\nIdentifying peaks: In larger datasets, modes reveal common values"
  },
  {
    "objectID": "lesson_03.html#comparing-mean-median-and-mode",
    "href": "lesson_03.html#comparing-mean-median-and-mode",
    "title": "Lesson 3: Measures of Location",
    "section": "5 Comparing Mean, Median, and Mode",
    "text": "5 Comparing Mean, Median, and Mode\n\n\n\n\n\nMeasure\nDefinition\nUses All Data?\nResistant to Outliers?\nWorks for Categorical?\nBest Used When\n\n\n\n\nMean\nArithmetic average\nYes\nNo\nNo\nSymmetric distribution, no outliers\n\n\nMedian\nMiddle value when sorted\nNo\nYes\nNo\nSkewed distribution or outliers present\n\n\nMode\nMost frequent value\nNo\nYes\nYes\nFinding most common category"
  },
  {
    "objectID": "lesson_03.html#percentiles-and-quartiles",
    "href": "lesson_03.html#percentiles-and-quartiles",
    "title": "Lesson 3: Measures of Location",
    "section": "6 Percentiles and Quartiles",
    "text": "6 Percentiles and Quartiles\nSometimes we want to know more than just the center. Percentiles describe the relative standing of values in a distribution.\n\n6.1 Percentiles\n\n\n\n\n\n\nDefinition: Percentile\n\n\n\nThe \\(p\\)th percentile is the value such that \\(p\\) percent of the observations fall at or below it.\nNotation: \\(P_p\\) denotes the \\(p\\)th percentile\n\n\n\n\n6.2 Example 3.6: Interpreting Percentiles\nIf your ACFT score is at the 85th percentile:\n\n85% of soldiers scored at or below your score\n15% of soldiers scored above your score\nYou performed better than 85% of test-takers\n\n\n\n6.3 Special Percentiles: Quartiles\nQuartiles divide the data into four equal parts:\n\n\\(Q_1\\) (First Quartile) = 25th percentile\n\\(Q_2\\) (Second Quartile) = 50th percentile = Median\n\\(Q_3\\) (Third Quartile) = 75th percentile\n\n\n\nCode\n# Generate sample ACFT scores\nacft_scores &lt;- rnorm(100, mean = 480, sd = 50)\n\n# Calculate quartiles\nquartiles &lt;- quantile(acft_scores, probs = c(0.25, 0.50, 0.75))\nquartiles\n\n\n     25%      50%      75% \n447.3926 483.9923 522.3987 \n\n\nInterpretation:\n\n25% of scores are below 447.4\n50% of scores are below 484 (the median)\n75% of scores are below 522.4\n\n\n\n6.4 Computing Percentiles in R\n\n\nCode\n# 90th percentile of ACFT scores\np90 &lt;- quantile(acft_scores, probs = 0.90)\np90\n\n\n     90% \n546.5145 \n\n\nCode\n# Multiple percentiles at once\npercentiles &lt;- quantile(acft_scores, probs = c(0.10, 0.25, 0.50, 0.75, 0.90))\npercentiles\n\n\n     10%      25%      50%      75%      90% \n429.6052 447.3926 483.9923 522.3987 546.5145"
  },
  {
    "objectID": "lesson_03.html#distribution-shape-and-measures-of-center",
    "href": "lesson_03.html#distribution-shape-and-measures-of-center",
    "title": "Lesson 3: Measures of Location",
    "section": "7 Distribution Shape and Measures of Center",
    "text": "7 Distribution Shape and Measures of Center\nThe relationship between mean and median reveals information about the distribution’s shape:\n\n7.1 Symmetric Distributions\n\n\n\n\n\n\n\n\n\nSymmetric: Mean ≈ Median\n\n\n7.2 Right-Skewed Distributions\n\n\n\n\n\n\n\n\n\nRight-skewed: Mean &gt; Median (pulled toward the tail)\n\n\n7.3 Left-Skewed Distributions\n\n\n\n\n\n\n\n\n\nLeft-skewed: Mean &lt; Median (pulled toward the tail)"
  },
  {
    "objectID": "lesson_03.html#choosing-the-right-measure",
    "href": "lesson_03.html#choosing-the-right-measure",
    "title": "Lesson 3: Measures of Location",
    "section": "8 Choosing the Right Measure",
    "text": "8 Choosing the Right Measure\n\n\n\n\n\n\nWhich Measure Should I Use?\n\n\n\nUse the MEAN when:\n\nDistribution is roughly symmetric\nNo extreme outliers\nYou need to use all data values\n\nUse the MEDIAN when:\n\nDistribution is skewed\nOutliers are present\nYou want a resistant measure\n\nUse the MODE when:\n\nData is categorical\nYou want the most typical category/value"
  },
  {
    "objectID": "lesson_03.html#real-data-example-exploring-acft-scores",
    "href": "lesson_03.html#real-data-example-exploring-acft-scores",
    "title": "Lesson 3: Measures of Location",
    "section": "9 Real Data Example: Exploring ACFT Scores",
    "text": "9 Real Data Example: Exploring ACFT Scores\nLet’s analyze a simulated dataset of ACFT scores:\n\n\nCode\n# Simulate ACFT scores for 100 soldiers\nset.seed(206)\nacft_data &lt;- tibble(\n  soldier_id = 1:100,\n  acft_score = pmax(360, pmin(600, rnorm(100, mean = 480, sd = 60)))\n)\n\n# Calculate all measures of location\nsummary_stats &lt;- acft_data %&gt;%\n  summarize(\n    n = n(),\n    Mean = round(mean(acft_score), 1),\n    Median = round(median(acft_score), 1),\n    Q1 = round(quantile(acft_score, 0.25), 1),\n    Q3 = round(quantile(acft_score, 0.75), 1),\n    Min = min(acft_score),\n    Max = max(acft_score)\n  )\n\nsummary_stats %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nn\nMean\nMedian\nQ1\nQ3\nMin\nMax\n\n\n\n\n100\n485.8\n484.8\n440.9\n530.9\n360\n600\n\n\n\n\n\nVisualization:\n\n\nCode\nggplot(acft_data, aes(x = acft_score)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  geom_vline(xintercept = mean(acft_data$acft_score),\n             color = \"red\", linewidth = 1.5, linetype = \"dashed\") +\n  geom_vline(xintercept = median(acft_data$acft_score),\n             color = \"darkgreen\", linewidth = 1.5, linetype = \"dotted\") +\n  annotate(\"text\", x = mean(acft_data$acft_score) + 15, y = 15,\n           label = \"Mean\", color = \"red\", size = 5) +\n  annotate(\"text\", x = median(acft_data$acft_score) - 15, y = 15,\n           label = \"Median\", color = \"darkgreen\", size = 5) +\n  labs(\n    title = \"Distribution of ACFT Scores\",\n    subtitle = paste(\"Mean =\", round(mean(acft_data$acft_score), 1),\n                     \"| Median =\", round(median(acft_data$acft_score), 1)),\n    x = \"ACFT Score\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "lesson_03.html#practice-problems",
    "href": "lesson_03.html#practice-problems",
    "title": "Lesson 3: Measures of Location",
    "section": "10 Practice Problems",
    "text": "10 Practice Problems\n\n10.1 Practice 3.1\nCalculate the mean, median, and mode for these datasets:\na) Push-up counts: 45, 50, 55, 50, 60, 50, 65\nb) Mile run times (minutes): 6.5, 7.0, 6.8, 7.2, 6.5, 8.5, 6.9\n\n\n10.2 Practice 3.2\nTen soldiers complete a 12-mile ruck march with the following times (in minutes):\n\\[140, 145, 150, 155, 160, 165, 170, 175, 180, 220\\]\n\nCalculate the mean and median\nWhich measure better represents the “typical” time? Why?\nIdentify the outlier. How does it affect each measure?\n\n\n\n10.3 Practice 3.3\nThe following are final exam scores for a class:\n\\[65, 72, 78, 80, 82, 85, 88, 90, 92, 95, 98\\]\n\nFind the 25th, 50th, and 75th percentiles\nWhat score is at the 90th percentile?\nIf a cadet scored 85, what percentile are they at approximately?\n\n\n\n10.4 Practice 3.4\nFor each scenario, determine whether mean or median would be more appropriate:\n\nAverage salary in the Army (including generals)\nTypical age of cadets at USMA\nAverage household income in the US\nTypical number of siblings for cadets"
  },
  {
    "objectID": "lesson_03.html#summary",
    "href": "lesson_03.html#summary",
    "title": "Lesson 3: Measures of Location",
    "section": "11 Summary",
    "text": "11 Summary\nKey Takeaways:\n\nMean \\((\\bar{x})\\): Arithmetic average, uses all data, sensitive to outliers\nMedian: Middle value, resistant to outliers, better for skewed distributions\nMode: Most frequent value, useful for categorical data\nPercentiles: Describe relative standing\n\n\\(Q_1\\) = 25th percentile\n\\(Q_2\\) = 50th percentile = Median\n\\(Q_3\\) = 75th percentile\n\nDistribution shape affects mean-median relationship:\n\nSymmetric: Mean ≈ Median\nRight-skewed: Mean &gt; Median\nLeft-skewed: Mean &lt; Median\n\n\nNext: In Lesson 4, we’ll learn about measures of variability to describe the spread of data."
  },
  {
    "objectID": "lesson_03.html#additional-resources",
    "href": "lesson_03.html#additional-resources",
    "title": "Lesson 3: Measures of Location",
    "section": "12 Additional Resources",
    "text": "12 Additional Resources\n\nDevore Section 1.3\nKhan Academy: Mean, Median, Mode\nWebAssign homework on measures of location"
  },
  {
    "objectID": "lesson_01.html",
    "href": "lesson_01.html",
    "title": "Lesson 1: Types of Data, Sampling & Study Design",
    "section": "",
    "text": "By the end of this lesson, you will be able to:\n\nDistinguish populations, samples, and processes in studies\nClassify variables as categorical or quantitative\nIdentify parameters vs. statistics in context\n\nReading: Devore 1.1"
  },
  {
    "objectID": "lesson_01.html#introduction",
    "href": "lesson_01.html#introduction",
    "title": "Lesson 1: Types of Data, Sampling & Study Design",
    "section": "1 Introduction",
    "text": "1 Introduction\nStatistical thinking begins with understanding what we’re measuring and how we collect that information. Before we can analyze data, we must clearly define:\n\nWhat entities we’re studying (observational units)\nWhat characteristics we’re measuring (variables)\nWhether we’re studying everyone or just a subset (population vs. sample)"
  },
  {
    "objectID": "lesson_01.html#populations-samples-and-processes",
    "href": "lesson_01.html#populations-samples-and-processes",
    "title": "Lesson 1: Types of Data, Sampling & Study Design",
    "section": "2 Populations, Samples, and Processes",
    "text": "2 Populations, Samples, and Processes\n\n2.1 Key Definitions\n\n\n\n\n\n\nFundamental Concepts\n\n\n\nPopulation: The entire collection of individuals or objects about which information is desired.\nSample: A subset of the population selected for study.\nParameter: A numerical characteristic of a population (usually unknown).\nStatistic: A numerical characteristic of a sample (computed from data).\n\n\n\n\n2.2 Why Study Samples?\nIn most real-world scenarios, studying an entire population is:\n\nImpractical - Too time-consuming or expensive\nImpossible - Population is infinite or inaccessible\nUnnecessary - A well-chosen sample provides reliable information\n\nThe key challenge: How do we ensure our sample accurately represents the population?\n\n\n2.3 Example 1.1: Population vs. Sample\nScenario: You want to know the average height of all cadets in the Corps.\n\nPopulation: All 4,400 cadets at USMA\nSample: 200 randomly selected cadets\nParameter: \\(\\mu\\) = true average height of all cadets (unknown)\nStatistic: \\(\\bar{x}\\) = average height of the 200 sampled cadets (computable)\n\nIf we calculate \\(\\bar{x} = 69.5\\) inches from our sample, we estimate that \\(\\mu \\approx 69.5\\) inches."
  },
  {
    "objectID": "lesson_01.html#types-of-variables",
    "href": "lesson_01.html#types-of-variables",
    "title": "Lesson 1: Types of Data, Sampling & Study Design",
    "section": "3 Types of Variables",
    "text": "3 Types of Variables\nVariables are characteristics that can differ from one observational unit to another. Understanding variable types determines which statistical methods we can use.\n\n3.1 Categorical Variables\nCategorical (qualitative) variables place individuals into groups or categories.\n\n3.1.1 Subtypes:\n\nNominal: Categories with no natural ordering\n\nExamples: Color (red, blue, green), Military Occupational Specialty (11A, 35D, 12A)\n\nOrdinal: Categories with a meaningful order\n\nExamples: Class rank (4th, 3rd, 2nd, 1st class), Likert scale (Strongly Disagree → Strongly Agree)\n\nBinary: Only two possible categories\n\nExamples: Pass/Fail, Yes/No, True/False\n\n\n\n\n\n3.2 Quantitative Variables\nQuantitative (numerical) variables take numerical values where arithmetic operations make sense.\n\n3.2.1 Subtypes:\n\nDiscrete: Countable values (often integers)\n\nExamples: Number of pushups, number of siblings, count of vehicles\n\nContinuous: Can take any value in an interval\n\nExamples: Height, weight, temperature, time\n\n\n\n\n\n3.3 Quick Classification Guide\n\n\n\n\n\nQuestion\nIf Yes\nIf No\n\n\n\n\nCan you perform arithmetic?\nQuantitative\nCategorical\n\n\nAre values countable?\nDiscrete\nContinuous\n\n\nIs there a natural order?\nOrdinal\nNominal\n\n\n\n\n\n\n\n3.4 Example 1.2: Classifying Variables\nLet’s classify variables from a cadet fitness study:\n\n\n\n\n\nVariable\nType\nReasoning\n\n\n\n\nAPFT Score\nQuantitative (Discrete)\nCounts total points (0-300)\n\n\nGender\nCategorical (Binary)\nTwo categories: Male/Female\n\n\nCompany\nCategorical (Nominal)\nNo natural ordering of A-I companies\n\n\n2-Mile Run Time (minutes)\nQuantitative (Continuous)\nCan take any positive value\n\n\nNumber of Pull-ups\nQuantitative (Discrete)\nCount of repetitions\n\n\nFitness Category (Excellent/Good/Fair/Poor)\nCategorical (Ordinal)\nOrdered quality levels"
  },
  {
    "objectID": "lesson_01.html#working-with-real-data-in-r",
    "href": "lesson_01.html#working-with-real-data-in-r",
    "title": "Lesson 1: Types of Data, Sampling & Study Design",
    "section": "4 Working with Real Data in R",
    "text": "4 Working with Real Data in R\nLet’s explore variable types using a dataset. We’ll use R’s built-in mtcars dataset about automobiles.\n\n\nCode\n# Load the dataset\ndata(mtcars)\n\n# View the first few rows\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n4.1 Identifying Variable Types\n\n\nCode\n# Structure of the dataset\nstr(mtcars)\n\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nLet’s classify each variable:\n\n\nCode\n# Create a summary of variable types\nmtcars_summary &lt;- tibble(\n  Variable = names(mtcars),\n  Type = c(\n    \"Quantitative (Continuous)\",  # mpg\n    \"Quantitative (Discrete)\",     # cyl\n    \"Quantitative (Continuous)\",   # disp\n    \"Quantitative (Continuous)\",   # hp\n    \"Quantitative (Continuous)\",   # drat\n    \"Quantitative (Continuous)\",   # wt\n    \"Quantitative (Continuous)\",   # qsec\n    \"Categorical (Binary)\",        # vs (engine type)\n    \"Categorical (Binary)\",        # am (transmission)\n    \"Quantitative (Discrete)\",     # gear\n    \"Quantitative (Discrete)\"      # carb\n  )\n)\n\nmtcars_summary %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nVariable\nType\n\n\n\n\nmpg\nQuantitative (Continuous)\n\n\ncyl\nQuantitative (Discrete)\n\n\ndisp\nQuantitative (Continuous)\n\n\nhp\nQuantitative (Continuous)\n\n\ndrat\nQuantitative (Continuous)\n\n\nwt\nQuantitative (Continuous)\n\n\nqsec\nQuantitative (Continuous)\n\n\nvs\nCategorical (Binary)\n\n\nam\nCategorical (Binary)\n\n\ngear\nQuantitative (Discrete)\n\n\ncarb\nQuantitative (Discrete)\n\n\n\n\n\n\n\n4.2 Converting Variables in R\nSometimes R doesn’t automatically recognize categorical variables. We can convert them:\n\n\nCode\n# Convert categorical variables to factors\nmtcars_clean &lt;- mtcars %&gt;%\n  mutate(\n    cyl = factor(cyl),\n    vs = factor(vs, labels = c(\"V-shaped\", \"Straight\")),\n    am = factor(am, labels = c(\"Automatic\", \"Manual\")),\n    gear = factor(gear),\n    carb = factor(carb)\n  )\n\n# Check the structure again\nstr(mtcars_clean)\n\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : Factor w/ 3 levels \"4\",\"6\",\"8\": 2 2 1 2 3 2 3 1 1 2 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : Factor w/ 2 levels \"V-shaped\",\"Straight\": 1 1 2 2 1 2 1 2 2 2 ...\n $ am  : Factor w/ 2 levels \"Automatic\",\"Manual\": 2 2 2 1 1 1 1 1 1 1 ...\n $ gear: Factor w/ 3 levels \"3\",\"4\",\"5\": 2 2 2 1 1 1 1 2 2 2 ...\n $ carb: Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 4 1 1 2 1 4 2 2 4 ..."
  },
  {
    "objectID": "lesson_01.html#parameters-vs.-statistics-notation-matters",
    "href": "lesson_01.html#parameters-vs.-statistics-notation-matters",
    "title": "Lesson 1: Types of Data, Sampling & Study Design",
    "section": "5 Parameters vs. Statistics: Notation Matters",
    "text": "5 Parameters vs. Statistics: Notation Matters\nStatistical inference involves using sample statistics to make claims about population parameters. We use different notation for each:\n\n\n\n\n\nMeasure\nPopulation\nSample\n\n\n\n\nMean\n$\\mu$ (mu)\n$\\bar{x}$ (x-bar)\n\n\nProportion\n$\\pi$ or $p$\n$\\hat{p}$ (p-hat)\n\n\nStandard Deviation\n$\\sigma$ (sigma)\n$s$\n\n\nVariance\n$\\sigma^2$\n$s^2$\n\n\nCorrelation\n$\\rho$ (rho)\n$r$\n\n\nSize\n$N$\n$n$\n\n\n\n\n\n\n\n\n\n\n\nCommon Mistake\n\n\n\nDon’t confuse population proportion \\(p\\) with p-value! Context matters.\n\nWhen discussing “true proportion of successes” → population parameter \\(p\\) or \\(\\pi\\)\nWhen discussing “probability of observing data if null hypothesis is true” → p-value\n\n\n\n\n5.1 Example 1.3: Parameters and Statistics\nScenario: The Army wants to know what percentage of all soldiers prefer virtual training to in-person training.\n\nPopulation: All active-duty soldiers\nParameter of interest: \\(\\pi\\) = proportion of ALL soldiers who prefer virtual training\nSample: 1,000 randomly selected soldiers\nStatistic: \\(\\hat{p} = 0.62\\) (62% of sampled soldiers prefer virtual)\n\nInference: We use \\(\\hat{p} = 0.62\\) to estimate that \\(\\pi \\approx 0.62\\).\nLater in the course, we’ll learn how to quantify our uncertainty about this estimate using confidence intervals."
  },
  {
    "objectID": "lesson_01.html#practice-problems",
    "href": "lesson_01.html#practice-problems",
    "title": "Lesson 1: Types of Data, Sampling & Study Design",
    "section": "6 Practice Problems",
    "text": "6 Practice Problems\n\n6.1 Practice 1.1\nFor each scenario, identify:\n\nThe population\nThe sample (if applicable)\nThe parameter of interest\nAny statistics mentioned\n\nScenario 1: A study of 500 West Point graduates found that 73% were promoted to Major within 10 years of commissioning.\nScenario 2: The registrar wants to know the average GPA of all cadets in the Class of 2026.\nScenario 3: All members of a particular company took the ACFT. The company commander calculates the average score.\n\n\n6.2 Practice 1.2\nClassify each variable as categorical or quantitative. If categorical, specify nominal/ordinal/binary. If quantitative, specify discrete/continuous.\n\nRank (CPT, MAJ, LTC, COL)\nBlood pressure (mmHg)\nNumber of deployments\nBranch (Infantry, Armor, Signal, etc.)\nRuck march time (minutes)\nGraduating class rank (1-1000)\nState of legal residence\nCombat patch (Yes/No)"
  },
  {
    "objectID": "lesson_01.html#summary",
    "href": "lesson_01.html#summary",
    "title": "Lesson 1: Types of Data, Sampling & Study Design",
    "section": "7 Summary",
    "text": "7 Summary\nIn this lesson, we established the foundation for statistical thinking:\n\nPopulations contain all individuals of interest; samples are subsets we actually study\nParameters describe populations (unknown); statistics describe samples (computed from data)\nCategorical variables group individuals into categories (nominal, ordinal, binary)\nQuantitative variables measure numerical quantities (discrete, continuous)\n\nUnderstanding these distinctions is critical for:\n\nChoosing appropriate statistical methods\nCorrectly interpreting results\nCommunicating findings clearly\n\nIn Lesson 2, we’ll explore how to collect samples and design studies to ensure our conclusions are valid."
  },
  {
    "objectID": "lesson_01.html#additional-resources",
    "href": "lesson_01.html#additional-resources",
    "title": "Lesson 1: Types of Data, Sampling & Study Design",
    "section": "8 Additional Resources",
    "text": "8 Additional Resources\n\nDevore Section 1.1\nR for Data Science - Data Transformation\nPractice problems in WebAssign"
  },
  {
    "objectID": "course_guide/index.html",
    "href": "course_guide/index.html",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "",
    "text": "To Cadets:\nThis course, MA206, introduces you to the foundational principles of probability and statistics, emphasizing data literacy and inference. It begins with Block I, covering data types, visualization, and basic probability rules including counting and the behavior of random variables. Block II builds on this by exploring discrete and continuous distributions, the Central Limit Theorem, and tools for one-sample inference such as confidence intervals and hypothesis testing for proportions and means. Finally, Block III develops cadets’ ability to analyze relationships between variables through two-sample tests, linear regression, ANOVA, and goodness-of-fit testing. By the end of the course, cadets will be equipped to make sound, data-driven decisions grounded in statistical reasoning.\nEvery statistical analysis begins with a clear, repeatable process that transforms curious questions into reliable insights. The Six Step Method serves as your roadmap—guiding you from formulating a meaningful research question through data collection, exploration, inference, and finally thoughtful reflection—so that every conclusion you draw is both rigorous and reproducible.\nSix Step Method\nA Note on Technology:\nIn this course the primary tool used for data analysis is R. Throughout this course you will implement techniques for summarizing, visualizing, and analyzing data. The primary focus of this course is not for you to become masters in coding, however building on skills learned in CY105 will help your analysis in understanding how to use information technology to demonstrate successful outcomes in this course."
  },
  {
    "objectID": "course_guide/index.html#types-of-data-sampling-and-bias",
    "href": "course_guide/index.html#types-of-data-sampling-and-bias",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "1.1 Types of Data, Sampling, and Bias",
    "text": "1.1 Types of Data, Sampling, and Bias\nFundamental to statistical analysis is understanding the types of data we encounter, the methods we use to collect them, and the potential sources of bias that can undermine the validity of our conclusions. We distinguish between categorical (qualitative) and quantitative (numerical) data. Categorical variables can further be broken down into nominal (color, baseball position, type of animal), ordinal (I had a very bad/somewhat bad/neutral/good/very good experience at CFT), and binary (Yes I passed Air Assault/No I did not). Quantitative data can also be broken down into either discrete or continuous. Understanding these distinctions is critical for selecting the correct tools for analysis and interpretation.\nThe two methods of sampling in this course will be through simple random sampling or convenience sampling. The difference in application is whether we can generalize our results to the larger population. As an example, say I do not have an exhaustive list of cadet ID numbers to randomly select from. Instead, I only can stand in Central Area after class at 1630 and survey the first 50 cadets that willingly take my survey. There is a certain sub-population I am probably missing (1st Reg, 4th Reg, Corps Squad, etc). This is a convenience sample. Instead, if I randomly select 50 C-Numbers from a complete list of the Corps obtained from the registrar, this would be a a true random sample of the Corps, whereas the former is what is known as selection bias.\nFinally, we explore the concept of bias in data collection. We identify common sources such as selection bias, response bias, and measurement bias, and discuss how poor sampling practices or flawed survey design can distort findings. This lesson sets the stage for the rest of the course by highlighting the importance of thoughtful data collection and critical evaluation of data sources.\nAs an example let us look at a dataset aggregated from over 50,000 diamonds:\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n1.02\nGood\nG\nVS2\n63.8\n59.0\n6080\n6.34\n6.27\n4.02\n\n\n0.31\nIdeal\nF\nVVS1\n61.9\n53.5\n882\n4.36\n4.39\n2.71\n\n\n0.60\nPremium\nD\nSI2\n61.3\n61.0\n1428\n5.46\n5.40\n3.33\n\n\n0.41\nIdeal\nE\nIF\n62.1\n54.0\n1419\n4.75\n4.81\n2.97\n\n\n0.72\nVery Good\nH\nVS1\n62.2\n54.0\n2877\n5.74\n5.76\n3.57\n\n\n1.20\nIdeal\nF\nVS2\n62.6\n56.0\n8486\n6.78\n6.73\n4.23\n\n\n\n\n\nEach of the rows is an individual diamond, generally called an observation. Each of the columns are unique aspects measured for every observation, called variables. Variables are either categorical, qualitative aspects of each measurement, or quantitative, a numbered entry."
  },
  {
    "objectID": "course_guide/index.html#exploratory-data-analysis",
    "href": "course_guide/index.html#exploratory-data-analysis",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "1.2 Exploratory Data Analysis",
    "text": "1.2 Exploratory Data Analysis\nUnderstanding, communicating, and interpreting your data is paramount to any initial data analysis project. These are done through numerous visualizations and summary statistics which we will learn to regularly implement when given any new dataset.\n\n1.2.1 One Variable – Visualizations and Summary Statistics\nStarting with a variable-by-variable approach is a natural first step. This is done rapidly in R with the following few functions:\n\n1.2.1.1 Histograms\n\n\n\n\n\n\n\n\n\nHistograms tell us where most of the values for a quantitative variable lie in its given distribution. We can determine skewness, a measure of how lopsided the data appear or if there are any asymmetries or tails.\n\n\n1.2.1.2 Boxplots\n\n\n\n\n\n\n\n\n\nSimilar to a histogram, a boxplot will tell us exactly where the median, 1st and 3rd quartiles, and outliers exist for any quantitative variable. The ‘whiskers’ are determined by \\(1.5 \\times IQR\\) where the inter-quartile range is the \\(3rd - 1st\\) quartiles.\n\n\n1.2.1.3 Summary Statistics\n\n\n\n\n\nVariable\nMean\nMedian\nSD\nVar\nMin\nMax\n\n\n\n\ncarat\n0.79\n0.70\n0.46\n0.21\n0.23\n3.24\n\n\ndepth\n61.77\n61.90\n1.40\n1.97\n56.30\n68.90\n\n\ntable\n57.48\n57.00\n2.26\n5.11\n50.00\n66.00\n\n\nprice\n3873.55\n2387.00\n3912.55\n15308046.06\n337.00\n18692.00\n\n\nx\n5.72\n5.68\n1.10\n1.21\n3.88\n9.44\n\n\ny\n5.72\n5.68\n1.09\n1.20\n3.90\n9.40\n\n\nz\n3.53\n3.52\n0.68\n0.46\n2.39\n5.85\n\n\n\n\n\nThe above are the predominant statistics you want to discern for every quantitative variable in your dataset. The benchmark location statistics are the mean, median, max, and min, while the standard deviation and variance are measures of how spread out the data are relative to one another.\n\\[\n\\begin{align}\n\\text{Sample Mean: } \\ \\ & \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n x_i \\\\\n\\text{Sample Variance: } \\ \\ &  S^2 = \\frac{1}{n-1}\\sum_{i=1}^n ( x_i - \\bar{X}  )^2 \\\\\n\\text{Sample Standard Deviation:} \\ \\ & s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n ( x_i - \\bar{X} )^2}\n\\end{align}\n\\]\nNote: the standard deviation is the square root of the variance\n\n\n\n1.2.2 Two Variables – Visualizations and Summary Statistics\n\n1.2.2.1 Scatter Plots\n\n\n\n\n\n\n\n\n\nA scatterplot is the main tool to visualize and identify a relationship between two quantitative variables. Oftentimes, coloring each observation by another categorical variable is a way to maximize effectiveness of a single plot, as you are encoding more information within the same space.\n\n\n1.2.2.2 Bar Plots\n\n\n\n\n\n\n\n\n\nBar charts are visualizations when focusing your audience on a single categorical and a summary statistic or aspect of a quantitative variable. In practice, scatter-plots show more information since you can encode the categorical variable through another channel such as color, size, shape while keeping the horizontal and vertical axis for two separate quantitative variables.\n\n\n1.2.2.3 Correlation\n\n\n\n\n\n\ncarat\ndepth\ntable\nprice\n\n\n\n\ncarat\n1.00\n0.00\n0.22\n0.91\n\n\ndepth\n0.00\n1.00\n-0.32\n-0.02\n\n\ntable\n0.22\n-0.32\n1.00\n0.17\n\n\nprice\n0.91\n-0.02\n0.17\n1.00\n\n\n\n\n\nCorrelation is the only multivariate summary statistic we will be using in this course, used to describe how two variables tend to move in tandem with one another. A perfect linear association evokes a correlation of 1, the opposite being a perfect negative association with a correlation of -1. No association is implied by a correlation near 0.\nMathematically: &gt; Definition &gt; For any two variables X,Y, the correlation of X and Y are: \\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\]"
  },
  {
    "objectID": "course_guide/index.html#probability",
    "href": "course_guide/index.html#probability",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "1.3 Probability",
    "text": "1.3 Probability\n\n1.3.1 Sample Space and Random Experiment (\\(\\Omega\\))\nA random experiment is a process that produces an outcome which cannot be predicted with certainty in advance. It must be well-defined, have more than one possible outcome, and be repeatable under similar conditions. Each performance of the experiment results in a single outcome from the sample space. The sample space is the set of all possible outcomes of a random experiment.\n\n\nDefinition:\n\nThe sample space is the set of all possible outcomes of a random experiment, denoted \\(\\Omega\\).\nAn event is a subset of the sample space. It can represent one or more outcomes.\nIf all outcomes in \\(\\Omega\\) are equally likely, then for any event \\(A\\):\n\n\n\\[\n\\mathbb{P}(A) = \\frac{\\text{Number of outcomes in } A}{\\text{Total outcomes in } \\Omega}\n\\]\n\nExamples:\n\nTossing a coin once: \\(\\Omega = {\\text{Heads}, \\text{Tails}}\\)\nRolling a 6-sided die: \\(\\Omega = {1, 2, 3, 4, 5, 6}\\)\nLetter grade in MA206: \\(\\Omega = {A, B, C, D, F}\\)\nNumber of emails received in an hour: \\(\\Omega = {0, 1, 2, \\dots}\\)\n\n\n\n\n1.3.2 Probability Measure (\\(\\mathbb{P}\\))\nA probability measure is a rule, denoted \\(\\mathbb{P}\\), that assigns a number between 0 and 1 to every event in a collection of events (called a sigma-algebra, denoted \\(\\mathcal{F}\\) ). These probabilities must follow three key rules, known as the axioms of probability.\n\n\n\n\n\n\n\nAxioms of Probability\n\n\n\n\nNon-Negativity\nFor any event \\(A\\), the probability is never negative:\n\\[\n\\mathbb{P}(A) \\geq 0\n\\]\nNormalization\nThe probability of one of the events happening over the entire sample space is 1:\n\\[\n\\mathbb{P}(\\Omega) = 1\n\\]\nAdditivity (for disjoint events)\nIf events \\(A_1, A_2, A_3, \\dots\\) are mutually exclusive (no overlap), then the probability that any one of them occurs is the sum of their individual probabilities:\n\\[\n\\mathbb{P}\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\mathbb{P}(A_3) + \\cdots\n\\]\n\n\n\n\nExample (Simple):\nLet \\(A\\), \\(B\\), and \\(C\\) be outcomes when rolling a die:\n\n\\(A = \\{1\\}\\), $B = {3} $, \\(C = \\{5\\}\\)\n\nThese are disjoint events (they don’t overlap).\n\nThen: \\[\n\\mathbb{P}(A \\cup B \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\mathbb{P}(C) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}\n\\]\n\nThese three rules form the mathematical foundation of all probability calculations — everything else builds on them.\n\n\n\n\n\n\n\nSet Theory\n\n\n\n\nThe complement of an event \\(A\\), written \\(A^c\\), consists of all outcomes in \\(\\Omega\\) that are not in \\(A\\):\n\n\\[\n\\mathbb{P}(A) + \\mathbb{P}(A^c) = 1\n\\]\n\nThe intersection \\(A \\cap B\\) consists of outcomes where both \\(A\\) and \\(B\\) occur.\nThe union \\(A \\cup B\\) consists of outcomes where either \\(A\\), \\(B\\), or both occur:\n\n\\[\n\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\n\\]\n\nTwo events \\(A\\) and \\(B\\) are disjoint (mutually exclusive) if they cannot both occur:\n\n\\[\nA \\cap B = \\varnothing \\quad \\text{and} \\quad \\mathbb{P}(A \\cap B) = 0\n\\]\n\n\n\n\n1.3.3 Conditional Probability\nFor events \\(A\\) and \\(B\\) with \\(0 &lt; P(B) \\le 1\\), the conditional probability of \\(A\\) given \\(B\\) is:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nExample: One card is drawn from a standard deck.\nLet \\(A\\): card is a Queen, and \\(B\\): card is a face card.\nFind \\(P(A)\\), \\(P(B)\\), and \\(P(A \\mid B)\\).\n\n\n\n1.3.4 Law of Total Probability\n\n\n\n\n\n\nLoTP\n\n\n\nIf \\(E_1, \\dots, E_n\\) is a partition of the sample space (mutually exclusive and exhaustive), then for any event \\(A\\):\n\\[\nP(A) = \\sum_{i=1}^{n} P(E_i) P(A \\mid E_i)\n\\]\n\n\nExample:\nA fair die is rolled. Let event A: “an even number is rolled”.\nLet:\n- \\(E_1\\): roll is 1 or 2\n- \\(E_2\\): roll is 3 or 4\n- \\(E_3\\): roll is 5 or 6\n\nFind:\n- \\(P(A \\mid E_1)\\), \\(P(A \\mid E_2)\\), \\(P(A \\mid E_3)\\)\n- Then use the Law of Total Probability to find \\(P(A)\\)\n\n\n\n1.3.5 Bayes’ Theorem\n\n\n\n\n\n\nSwitching the Conditioning of Events\n\n\n\n\nDefinition:\nIf \\(E_1, \\dots, E_n\\) is a partition of the sample space and \\(P(A) &gt; 0\\), then:\n\n\\[\nP(E_k \\mid A) = \\frac{P(A \\mid E_k) P(E_k)}{\\sum_{i=1}^n P(A \\mid E_i) P(E_i)}\n\\]\n\n\nExample:\nTwo urns:\n- Urn 1: 1 red, 1 blue\n- Urn 2: 3 red, 1 blue\nPick an urn at random, then draw one ball.\nIf the ball is red, what is the probability it came from Urn 1?\n\n\n\n1.3.6 Counting Principles\nBefore we can begin a thorough treatment of probability, some concepts in counting are needed to identify four common situations. These arise depending on when things are “allowed” to repeat or the “order” items are chosen in matters. The ability to discern when these four situations arise is more than half the battle.\n\n1.3.6.1 Ordered with Replacement\nThink of the number of ways of choosing a 4-digit passcode on your phone.\nThe order of the numbers matters, and you are allowed to repeat the same number. So how many arrangements are there? Since repetition is allowed and order matters, there are 10 digits for each position, giving:\n\\[\n\\text{Ordered Arrangements with Replacement} = n^r = 10^4 = 10{,}000\n\\]\n\n\n1.3.6.2 Ordered without Replacement\nThink of the number of ways I can create a batting order from 9 position players.\nThe order still matters, but players cannot be repeated. This is a permutation — an ordered arrangement without replacement.\n\\[\n{}_nP_r = P(n, r) = \\frac{n!}{(n - r)!}\n\\]\nNote: Factorial in math is computed as \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\).\nFor example, the number of ways to assign the first 3 batting positions from 9 players:\n\\[\n{}_9P_3 = \\frac{9!}{(9-3)!} = 9 \\times 8 \\times 7 = 504\n\\]\n\n\n1.3.6.3 Unordered without Replacement\nThink of how many ways you can choose 3 scoops of ice cream from 5 unique flavors without repeats.\nBecause order doesn’t matter and repeats aren’t allowed, we use combinations:\n\\[\n{}_nC_k = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\n\n\n1.3.6.4 Unordered with Replacement\nThink of how many different combinations of 3 scoop ice cream cones you can make with 5 unique flavors while allowing repeats.\nThis is the trickiest scenario. The order doesn’t matter, and repetition is allowed. The formula is:\n\\[\n\\text{Unordered Arrangements with Replacement} = \\binom{r+n-1}{r} = \\frac{(r+n-1)!}{r!(n-1)!}\n\\]\nExample: choosing 3 scoops from 5 flavors (with repeats):\n\\[\n\\binom{3+5-1}{3} = \\binom{7}{3} = 35\n\\]\nThis can be understood using the stars and bars method: selecting \\(r\\) scoops with \\(n-1\\) dividers. Imagine representing each scoop as a ★ (star) and using vertical bars | to separate flavor types. To choose \\(r\\) scoops from \\(n\\) flavors, you need \\(r\\) stars (for the scoops) and \\(n - 1\\) bars (to divide them into \\(n\\) categories). For example, if \\(r = 3\\) scoops and \\(n = 5\\) flavors, you arrange 3 stars and 4 bars in a row. One possible arrangement is ★ | ★★ | | |, which represents 1 scoop of flavor 1, 2 scoops of flavor 2, and 0 scoops of flavors 3, 4, and 5. The number of such arrangements is given by the combination formula \\(\\binom{r + n - 1}{r}\\), since you are choosing positions for the \\(r\\) indistinguishable stars among the \\(r + n - 1\\) total positions (stars and bars combined).\nNote: The above section may seem like it came out of nowhere, that is okay. A fundamental difficulty in probability is finding the sample space or event space due to finding the various different combinations/permutations sequences of different possibilities. To elaborate consider the next example:\n\nExample: No Matching Pairs in a Shoe Sample\nA closet contains \\(n\\) pairs of shoes (so \\(2n\\) total shoes). If \\(2r\\) shoes are chosen at random (where \\(2r &lt; n\\)), what is the probability that no matching pair is selected?\nWe are selecting \\(2r\\) shoes such that no left and right shoe from the same pair are both chosen.\nStrategy:\n1. First choose \\(2r\\) distinct pairs from the \\(n\\) available — there are \\(\\binom{n}{2r}\\) ways to do this.\n2. From each of these \\(2r\\) selected pairs, choose only one shoe (either left or right) — there are \\(2^{2r}\\) ways to do this.\n3. The total number of ways to choose any \\(2r\\) shoes out of \\(2n\\) is \\(\\binom{2n}{2r}\\).\nSo, the desired probability is:\n\\[\n\\mathbb{P}(\\text{No matching pair}) = \\frac{\\binom{n}{2r} \\cdot 2^{2r}}{\\binom{2n}{2r}}\n\\]"
  },
  {
    "objectID": "course_guide/index.html#random-variables-expectation-and-variance",
    "href": "course_guide/index.html#random-variables-expectation-and-variance",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "2.1 Random Variables, Expectation, and Variance",
    "text": "2.1 Random Variables, Expectation, and Variance\n\n\n\n\n\n\nRandom Variable\n\n\n\nA random variable is a mapping that assigns a real number to every outcome in the sample space:\n\\[\nX: \\Omega \\rightarrow \\mathbb{R}\n\\]\n\n\n\n\n\n\n\n\nCumulative Distribution Function\n\n\n\nA cumulative distribution function (CDF) is a function \\(F_X: \\mathbb{R} \\rightarrow [0,1]\\) defined by:\n\\[\nF_X(x) = \\mathbb{P}(X \\le x)\n\\]"
  },
  {
    "objectID": "course_guide/index.html#discrete-random-variables",
    "href": "course_guide/index.html#discrete-random-variables",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "2.2 Discrete Random Variables",
    "text": "2.2 Discrete Random Variables\n\n\n\n\n\n\nDiscrete Random Variables\n\n\n\nA discrete random variable is a random variable that takes countably many values in \\(\\mathbb{R}\\).\nIts probability mass function is given by:\n\\[\nf_X(x) = \\mathbb{P}(X = x)\n\\]\nThe expected value (mean) of a discrete random variable \\(X\\) is:\n\\[\n\\mathbb{E}[X] = \\sum_x x \\cdot \\mathbb{P}(X = x)\n\\]\nDefinition (Variance):\nThe variance of a discrete random variable \\(X\\) is:\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = x)\n\\]\n\n\n\n2.2.1 Binomial Distribution:\n\nLet \\(X \\sim \\text{Binomial}(n, p)\\) where \\(n \\in \\mathbb{N}\\) and \\(0 &lt; p &lt; 1\\).\n\nProbability Mass Function (PMF):\n\\[\n\\mathbb{P}(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}, \\quad \\text{for } k = 0, 1, \\dots, n\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = \\sum_{k=0}^{\\lfloor x \\rfloor} \\binom{n}{k} p^k (1 - p)^{n - k}\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = np\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = np(1 - p)\n\\]\n\n\n\n\n2.2.2 Geometric Distribution:\n\nLet \\(X \\sim \\text{Geometric}(p)\\) be the number of trials until the first success (including the success), where \\(0 &lt; p &lt; 1\\).\n\nProbability Mass Function (PMF):\n\\[\n\\mathbb{P}(X = k) = (1 - p)^{k - 1} p, \\quad \\text{for } k = 1, 2, 3, \\dots\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = 1 - (1 - p)^{\\lfloor x \\rfloor}\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\frac{1}{p}\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\frac{1 - p}{p^2}\n\\]"
  },
  {
    "objectID": "course_guide/index.html#continuous-random-variables",
    "href": "course_guide/index.html#continuous-random-variables",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "2.3 Continuous Random Variables",
    "text": "2.3 Continuous Random Variables\n\n\n\n\n\n\nContinuous Random Variable\n\n\n\nA continuous random variable takes infinitely many values in \\(\\mathbb{R}\\). Its probability density function is given by:\n\\[\nf_X(x) = \\mathbb{P}(X = x)\n\\]\nNote: We do not find probabilities with the pdf like the pmf of a discrete random variable. We integrate over a neighborhood of the support of X, as there is no probability mass at any single point for a continuous distribution.\nThe expected value (mean) of a continuous random variable \\(X\\) is:\n\\[\n\\mathbb{E}[X] = \\int_{-\\infty}^\\infty x f_X(x)dx\n\\]\nDefinition (Variance):\nThe variance of a discrete random variable \\(X\\) is:\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right] = \\int_{-\\infty}^\\infty (x - \\mathbb{E}[X])^2 f_X(x)dx = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n\\]\n\n\n\n2.3.1 Central Limit Theorem (CLT)\nThe Central Limit Theorem (CLT) is one of the most important results in statistics. It states that the sampling distribution of the sample mean \\(\\bar{X}\\) becomes approximately normal as the sample size \\(n\\) increases, regardless of the shape of the population distribution (provided it has finite mean and variance).\nSpecifically, if \\(X_1, X_2, \\dots, X_n\\) are i.i.d. random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then:\n\\[\n\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0,1) \\quad \\text{as } n \\to \\infty\n\\]\nThis justifies the widespread use of the normal distribution to approximate sample means in practice.\n\n\n\n\nCredit: The New York Times\n\n\n\n2.3.2 Normal Distribution\nThe above video showed you the importance of this distribution, also called a Gaussian Distribution. Many natural phenomena are normally distributed.\n\nNormal Distribution\nLet \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), where \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma &gt; 0\\).\n\nProbability Density Function (PDF):\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right), \\quad x \\in \\mathbb{R}\n\\]\nCumulative Distribution Function (CDF):\nThere is no closed-form expression, but it is denoted as:\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = \\Phi\\left( \\frac{x - \\mu}{\\sigma} \\right)\n\\]\nwhere \\(\\Phi\\) is the standard normal CDF.\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\mu\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\sigma^2\n\\]\n\n\n\n\n2.3.3 Exponential Distribution\nThis distribution is helpful to model continuous time-related events: time between system failures at a factory, time between phone calls at a call center, time between insurance claims received at a insurance firm. All these can be modeled with this useful continuous random variable.\n\nLet \\(X \\sim \\text{Exponential}(\\lambda)\\) with \\(\\lambda &gt; 0\\).\n\nProbability Density Function (PDF):\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = 1 - e^{-\\lambda x}, \\quad x \\ge 0\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\frac{1}{\\lambda}\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\frac{1}{\\lambda^2}\n\\]"
  },
  {
    "objectID": "course_guide/index.html#confidence-intervals",
    "href": "course_guide/index.html#confidence-intervals",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "2.4 Confidence Intervals",
    "text": "2.4 Confidence Intervals\nA confidence interval (CI) gives a range of plausible values for a population parameter based on a sample statistic. The general structure of any confidence interval is:\n\\[\n\\text{point estimate} \\ \\pm \\ \\text{margin of error}\n\\]\nMore specifically, for large samples or when the sampling distribution of the estimate is approximately normal:\n\\[\n\\text{CI} = \\hat{\\theta} \\ \\pm \\ z^* \\cdot \\text{SE}(\\hat{\\theta})\n\\]\nWhere:\n\n\\(\\hat{\\theta}\\) is the point estimate (e.g., \\(\\bar{x}\\) for the mean, \\(\\hat{p}\\) for a proportion)\n\\(z^*\\) is the critical value from the standard normal distribution (e.g., 1.96 for 95% confidence)\n\\(\\text{SE}(\\hat{\\theta})\\) is the standard error of the estimate\n\nThis structure applies to many common settings:\n\nCI for a population mean: \\[\n\\bar{x} \\pm z^* \\cdot \\frac{s}{\\sqrt{n}}\n\\]\nCI for a population proportion: \\[\n\\hat{p} \\pm z^* \\cdot \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]\n\n\n2.4.0.1 Interpretation:\n\n“We are 95% confident that the true population parameter lies within this interval.”\n\nThis does not mean there’s a 95% probability the parameter is in the interval — rather, it means that 95% of all intervals computed from repeated samples in this manner would contain the true parameter."
  },
  {
    "objectID": "course_guide/index.html#one-sample-hypothesis-testing",
    "href": "course_guide/index.html#one-sample-hypothesis-testing",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "2.5 One Sample Hypothesis Testing",
    "text": "2.5 One Sample Hypothesis Testing\nHypothesis testing is a formal method for making inferences about a population using sample data. The whole test aspect is questioning if the statistic your sample shows is significantly different than a certain value in question. You have two underlying premises, referred to as the null and alternative hypotheses. The null hypothesis assumes that there is no difference: the statistic from your value is the same as the value you are testing. The alternative conflicts the null and says they are different. The process involves:\n\nState the null and alternative hypotheses. There are three different variants to create your hypotheses statements depending on what the question being asked entails.\n\n\n\nGreater than Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &&gt; Value \\ in \\ Question\n\\end{align}\n\\]\nThe entire inference aspect of hypothesis testing is that you are using your sample statistic, a tangible aspect of your data, to make an argument about the population parameter, an entity that is unknown to you. This is why the hypotheses are written in terms of the parameter. You are testing whether an aspect or parameter about the population is greater than a benchmark value decided by you in advance.\n\nLess than Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &&lt; Value \\ in \\ Question\n\\end{align}\n\\]\nVery similarly, the less than hypothesis is also a one-sided hypothesis test in that you are only testing one side of the value, abeit this time if the population parameter is less than the tested value.\n\nNot equal to Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &\\neq Value \\ in \\ Question\n\\end{align}\n\\]\nThis is the only two-sided hypothesis test, denoted with the not equal to alternative hypothesis. Both sides must be accounted for in this test, and therefore as we will see shortly require more evidence for significance.\n\nChoose a significance level \\(\\alpha\\).\n\nThis is the threshold you will also decide to inform your certainty in your conclusions. As seen previously, unless you are finding the probability that something will happen in the entire sample space (which happens probability 1); ie, there are no absolutes. Therefore there is always a chance that your conclusion will be wrong. Here is where you decide how “often” you are willing to be wrong. Is it 1% of the time? 5% of the time? Think of the significance level as choosing the percentage of the time you are willing to be wrong in your conclusion, (I know this sounds weird). A common \\(\\alpha\\) is 5%.\n\nCompute the test statistic.\n\nCompute the relevant summary statistic. In this course you will either be calculating a sample proportion, denoted \\(\\hat{p}\\) if the variable of interest is categorical or the sample mean \\(\\bar{X}\\) if quantitative.\n\nStandardize the test statistic.\n\nThis step transforms your statistic so it can be treated as a random variable from a named distribution. For proportions this will be a Normal Random Variable, however if quantitative it will be a t-distributed random variable.\n\n\n\n\nCredit: 365 Data Science\n\nDetermine the p-value.\n\nOnce we have the standardized statistic, it can be treated as either a Standard Normal Random Variable or Student’s-t Random Variable. This is where the alternative hypotheses come in to play to determine the p-value: the probability that if the null hypothesis is indeed true you choose to make an argument supporting the alternative. To find the probability of a certain event happening in a continuous random variable, you integrate the probability density function with the limits of integration being the range of values the random variable could take. Both the Standard Normal and Student’s-t are continuous, so depending on your alternative hypothesis, your p-value is calculated by either of the following:\n\\[\n\\begin{align}\n\\text{Greater Hypothesis} &&&& \\text{Less than Hypothesis} &&&& \\text{Two Sided Hypothesis} \\\\\n\\mathbb{P}(X&gt;z) = \\int_z^\\infty f_X(x) dx &&&& \\mathbb{P}(X&lt;z) = \\int_{-\\infty}^z f_X(x)dx &&&& \\mathbb{P}(X&gt;z) = \\int_{|z|}^\\infty f_X(x)dx + \\int_{-\\infty}^{-|z|} f_X(x)dx\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\nMaking a conclusion based on comparison.\n\nOnce a p-value is obtained, reference it to the significance level chosen. If the p-value is greater than \\(\\alpha\\), you fail to reject the null hypothesis, if it is smaller, you reject the null hypothesis, and then state what that means in the context of the problem.\n\n\n2.5.1 Single Proportion\nWe conduct inference on a population proportion \\(\\pi\\) relative to a hypothesized value \\(\\pi_0\\). The test statistic is:\n\\[\nz = \\frac{\\hat{p} - \\pi}{\\sqrt{\\frac{\\pi(1 - \\pi)}{n}}}\n\\]\n\n2.5.1.1 Example: Test if more than 40% of diamonds are “Ideal” cut\n\n\nCode\n# Null hypothesis: pi = 0.40\npi &lt;- 0.4\nn &lt;- nrow(diamonds)\nphat &lt;- mean(diamonds$cut == \"Ideal\")\n\n# Test statistic and p-value\nz &lt;- (phat - pi) / sqrt(pi * (1 - pi) / n)\np_value &lt;- 1 - pnorm(z)\n\ncat(\"Z-statistic:\", round(z, 3), \"\\n\")\n\n\nZ-statistic: -0.22 \n\n\nCode\ncat(\"P-value:\", round(p_value, 4))\n\n\nP-value: 0.587\n\n\n\n\n\n\n2.5.2 Single Mean\nWe conduct inference on a population mean \\(\\mu\\) relative to a hypothesized value \\(\\mu_0\\). The test statistic is:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\n2.5.2.1 Example: Test if the average diamond price is $4000.\n\n\nCode\nmu0 &lt;- 4000\nx_bar &lt;- mean(diamonds$price)\ns &lt;- sd(diamonds$price)\nn &lt;- length(diamonds$price)\n\n# Test statistic and p-value\nt_stat &lt;- (x_bar - mu0) / (s / sqrt(n))\np_value &lt;- 2*(1 - pt(abs(t_stat), df = n - 1))\n\ncat(\"T-statistic:\", round(t_stat, 3), \"\\n\")\n\n\nT-statistic: -3.912 \n\n\nCode\ncat(\"P-value:\", round(p_value, 4))\n\n\nP-value: 1e-04\n\n\n\n\n\n2.5.3 Experimental Design: Power, Type I / Type II Error\nIn any hypothesis test, we face the possibility of making incorrect conclusions. These are formalized through Type I and Type II errors:\n\nType I Error (\\(\\alpha\\)): Rejecting the null hypothesis when it is actually true. This is controlled by the significance level of the test, often set to \\(\\alpha = 0.05\\).\nType II Error (\\(\\beta\\)): Failing to reject the null hypothesis when the alternative is actually true. This is harder to control and depends on the true parameter, sample size, and variance.\nPower of the Test: The probability of correctly rejecting the null hypothesis when the alternative is true: \\[\n\\text{Power} = 1 - \\beta\n\\]\n\nA powerful test detects meaningful effects and minimizes Type II error. Power increases when: - Sample size increases (\\(n \\uparrow\\)) - Effect size increases (true parameter is farther from null) - Variability decreases (standard deviation \\(\\downarrow\\)) - Significance level \\(\\alpha\\) increases (easier to reject null)\n\n\n\n\n\n\n\n\n\n\n\nThis diagram shows: - The blue curve is the null distribution (centered at 0). - The red dashed curve is the alternative distribution (shifted mean). - The dotted vertical line is the critical value (e.g., \\(z = 1.645\\) for \\(\\alpha = 0.05\\) in a one-sided test). - Area to the right of this cutoff under the null curve is \\(\\alpha\\). - Area to the left of this cutoff under the alternative curve is \\(\\beta\\). - The remaining area under the red curve (right tail) is power."
  },
  {
    "objectID": "course_guide/index.html#two-sample-hypothesis-testing",
    "href": "course_guide/index.html#two-sample-hypothesis-testing",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "3.1 Two Sample Hypothesis Testing",
    "text": "3.1 Two Sample Hypothesis Testing\n\n3.1.1 Difference of Proportions\nWe compare two population proportions to determine whether there is a significant difference between them. The test statistic is:\n\\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}\n\\]\nWhere \\(\\hat{p}\\) is the pooled proportion.\n\n3.1.1.1 Example: Are “Ideal” cuts more common among Premium vs. Good color grades?\n\n\nCode\n# Subset to just Premium and Good clarity levels\ndf &lt;- diamonds %&gt;% filter(color ==\"D\" | color==\"E\")\n\ntable(df$cut, df$color)\n\n\n           \n               D    E    F    G    H    I    J\n  Fair       163  224    0    0    0    0    0\n  Good       662  933    0    0    0    0    0\n  Very Good 1513 2400    0    0    0    0    0\n  Premium   1603 2337    0    0    0    0    0\n  Ideal     2834 3903    0    0    0    0    0\n\n\nCode\n# Create a binary outcome: Ideal or not\ndf &lt;- df %&gt;%\n  mutate(is_ideal = cut == \"Ideal\")\n\n# Proportions\np1 &lt;- mean(df$is_ideal[df$color == \"D\"])\np2 &lt;- mean(df$is_ideal[df$color == \"E\"])\nn1 &lt;- sum(df$color == \"D\")\nn2 &lt;- sum(df$color == \"E\")\n\n# Pooled proportion\nphat &lt;- (p1 * n1 + p2 * n2) / (n1 + n2)\n\n# Test statistic\nz &lt;- (p1 - p2) / sqrt(phat * (1 - phat) * (1 / n1 + 1 / n2))\np_value &lt;- 2 * (1 - pnorm(abs(z)))\n\ncat(\"Z-statistic:\", round(z, 3), \"\\n\")\n\n\nZ-statistic: 2.566 \n\n\nCode\ncat(\"P-value:\", round(p_value, 4))\n\n\nP-value: 0.0103\n\n\n\n\n\n\n3.1.2 Multiple Proportions (Chi-Square Test of Independence)\nUsed when comparing proportions across more than two groups.\n\n3.1.2.1 Example: Is cut independent of color?\n\n\nCode\ntbl &lt;- table(diamonds$cut, diamonds$color)\nchisq.test(tbl)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tbl\nX-squared = 310.32, df = 24, p-value &lt; 2.2e-16\n\n\nThis test checks whether the distribution of cut types is independent of the diamond color. A small p-value suggests a dependency.\n\n\n\n\n3.1.3 Difference of Means\nUsed to compare two independent sample means.\n\n3.1.3.1 Example: Is the average price different between “Ideal” and “Fair” cuts?\n\n\nCode\ndf &lt;- diamonds %&gt;% filter(cut %in% c(\"Ideal\", \"Fair\"))\n\nt.test(price ~ cut, data = df)\n\n\n\n    Welch Two Sample t-test\n\ndata:  price by cut\nt = 9.7484, df = 1894.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Fair and group Ideal is not equal to 0\n95 percent confidence interval:\n  719.9065 1082.5251\nsample estimates:\n mean in group Fair mean in group Ideal \n           4358.758            3457.542 \n\n\nThis performs a two-sample t-test, assuming unequal variances by default. The null hypothesis is that the means are equal.\n\n\n\n\n3.1.4 Paired Data\nIn paired designs, each observation in one group is paired with a related observation in the other. Since diamonds has no natural pairing, we’ll simulate a paired example.\n\n3.1.4.1 Example (Simulated): Price before and after resizing a set of diamonds\n\n\nCode\nset.seed(123)\n\n# Simulate paired prices: original and discounted\nn &lt;- 100\noriginal_price &lt;- sample(diamonds$price, n)\ndiscounted_price &lt;- original_price * runif(n, 0.85, 0.95)\n\nt.test(original_price, discounted_price, paired = TRUE)\n\n\n\n    Paired t-test\n\ndata:  original_price and discounted_price\nt = 8.9496, df = 99, p-value = 2.132e-14\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 342.2012 537.1645\nsample estimates:\nmean difference \n       439.6829 \n\n\nThis tests whether the mean price before and after a simulated discount differs significantly."
  },
  {
    "objectID": "course_guide/index.html#regression",
    "href": "course_guide/index.html#regression",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "3.2 Regression",
    "text": "3.2 Regression\nIn this section we continue our multivariate inference with creating models for the purpose of identifying significance between explanatory (predictor) variables and the response. The function used to create the model, \\(\\hat{y_i}=f(x_i)\\) will make predictions, known as fitted values. Do to the variability in the data, these fitted values will not exactly predict the response (ie. \\(y_i \\neq \\hat{y}\\)) for all values in the response. These errors are the deviations from the response and the fitted values and are referred as residuals, with notation \\(\\epsilon_i = y_i - \\hat{y}_i\\).\nTo assess how well a model performs, the residuals are summarized in a few different methods:\n\n3.2.0.1 Mean Absolute Deviation\nThe average magnitude of the residuals:\n\\[\nMAD = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n\\]\n\n\n3.2.0.2 Mean Squared Error:\nThe average magnitude of the residual-squared.\n\\[\nMSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\n\n\n3.2.1 Simple Linear Regression\nThe above metrics could be applied to any model, however the central method to assess a linear relationship between two quantitative variables isSimple Linear Regression, or better known as the line of best fit:\n\\[\n\\hat{y} = \\beta_0 + \\beta_1 x\n\\]\nThe \\(\\beta\\)’s are the parameters of the model: the y-intercept and slope. The reason the method is the best fit is because we optimizes the choices for these two parameters by minimizing the sum of squared error:\n\\[\n\\begin{align}\nSSE &= \\sum_{i=1}^n (\\epsilon_i)^2 \\\\\n    &= \\sum_{i=1}^n (y_i - \\widehat{y}_i)^2 \\\\\n    &= \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\\n\\frac{\\partial}{\\partial \\beta_0}SSE &= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i)^2 \\\\\n0 &= -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) \\\\\n\\widehat{\\beta}_0 &= \\bar{y} - \\widehat{\\beta}_1 \\bar{x} \\\\\n\\frac{\\partial}{\\partial \\beta_1}SSE &= \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i)^2 \\\\\n&= \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n (y_i - (\\bar{y} - \\widehat{\\beta}_1 \\bar{x}) - \\widehat{\\beta}_1x_i)^2 \\\\\n0 &= -2\\sum_{i=1}^n (\\bar{x} - x_i)(y_i - \\bar{y} + \\widehat{\\beta}_1( \\bar{x} - x_i)) \\\\\n\\widehat{\\beta}_1 \\sum_{i=1}^n (x_i - \\bar{x})^2 &= \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\\\\n\\widehat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)}\n\\end{align}\n\\]\nThe above is beyond the scope of this course, however it warrants a healthy appreciation for finding the line of best fit!\nIn practice, from the diamonds dataset we could model price as a function of carat:\n\n\nCode\n# Sample and fit model\nset.seed(206)\ndf &lt;- ggplot2::diamonds %&gt;% sample_n(1000)\nlm_simple &lt;- lm(price ~ carat, data = df)\nsummary(lm_simple)\n\n\n\nCall:\nlm(formula = price ~ carat, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8077.3  -813.1    10.2   607.5  8759.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2376.64      93.40  -25.45   &lt;2e-16 ***\ncarat        7885.65      99.37   79.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1499 on 998 degrees of freedom\nMultiple R-squared:  0.8632,    Adjusted R-squared:  0.8631 \nF-statistic:  6297 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nThere is a lot in the summary output, however the main ideas here lie in the magnitude and sign of the coefficient, looking for practical significance, and also looking at the size of the p-value relative to a chosen \\(\\alpha\\), checking for statistical significance.\nYou may be wondering in a line of best fit, where did the p-value come from? Good question! In addition to finding the line of best fit, our linear model assesses the relevance of all parameters in the model. This assessment is a *one-sample t-test for every ! If there is significance, then there is a significant assocation between the explanatory variable and the response.\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Multiple Linear Regression\nWe can extend linear regression to in fact include as many predictor variables as we want (as long as we have more observations than variables!). This is implemented through Multiple Linear Regression:\n\\[\n\\widehat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\]\nThe derivation would be too lengthy to do them individually previously, however through Matrix Algebra (MA371 anyone?!), the solved vector of coefficients, \\(\\widehat{\\beta}_{p\\times1}\\) has the following solution:\n\\[\n\\widehat\\beta_{p\\times 1} = (X_{p\\times n}^TX_{n \\times p})^{-1}X_{p \\times n}^T \\vec{y}_{n \\times 1}\n\\] Observe the subscripts for the dimensions of the matrices, ending with a \\(p \\times 1\\) vector for the coefficients that minimize the SSE.\nHere, we use carat(Q), depth(Q), table(Q), and color(Q) to predict price.\n\n\n\n\n\n\nNote\n\n\n\nWhen a categorical variable like color is included in a regression model, R converts it into multiple indicator (dummy) variables, one for each level except the reference level (usually the first alphabetically, unless manually changed).\nEach dummy variable shifts the intercept of the regression line for that level, while the slope(s) for the numeric predictors remain the same. This means:\n\nR is effectively fitting a separate line of best fit for each level of the categorical variable — all with the same slope, but different intercepts.\n\nFor example, including color in lm(price ~ carat + depth + table + color) produces:\n\nOne baseline line (intercept) for the reference group (e.g., color = \"D\").\nAdditional parallel lines for each other color group (e.g., E, F, G, etc.), each shifted vertically by its corresponding coefficient (e.g., colorE, colorF, etc.).\n\nSo if the coefficient for colorE is -500, then diamonds with color E are estimated to be $500 less expensive than D-colored diamonds at the same carat, depth, and table values.\nThis approach captures group differences in the starting point (intercepts) of the response, while assuming the effect of continuous predictors (e.g. carat) is constant across all groups.\n\n\n\n\nCode\nlm_multi &lt;- lm(price ~ carat + depth + table + color, data = df)\nsummary(lm_multi)\n\n\n\nCall:\nlm(formula = price ~ carat + depth + table + color, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9095.0  -787.8   -80.5   568.3  7873.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10739.61    2692.30   3.989 7.12e-05 ***\ncarat        8251.22      97.16  84.920  &lt; 2e-16 ***\ndepth        -112.34      33.40  -3.364 0.000799 ***\ntable        -115.68      20.77  -5.569 3.30e-08 ***\ncolor.L     -1555.67     146.23 -10.639  &lt; 2e-16 ***\ncolor.Q      -843.07     133.77  -6.303 4.40e-10 ***\ncolor.C       -85.92     129.08  -0.666 0.505778    \ncolor^4        75.10     117.70   0.638 0.523580    \ncolor^5      -123.89     116.66  -1.062 0.288500    \ncolor^6       -72.64     104.92  -0.692 0.488903    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1385 on 990 degrees of freedom\nMultiple R-squared:  0.8842,    Adjusted R-squared:  0.8832 \nF-statistic: 840.1 on 9 and 990 DF,  p-value: &lt; 2.2e-16\n\n\nYou can compare \\(R^2\\) values and p-values to determine whether the additional variables meaningfully improve the model.\n\n3.2.2.1 Adding Interaction Terms\nWhat if the effect of one variable depends on another? For example, maybe the impact of carat on price differs across color levels. In that case, we can include an interaction term in the model:\n\n\nCode\nlm_interact &lt;- lm(price ~ carat * color, data = df)\nsummary(lm_interact)\n\n\n\nCall:\nlm(formula = price ~ carat * color, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9236.8  -767.5   -13.5   648.9  8199.4 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2642.59      99.53 -26.551  &lt; 2e-16 ***\ncarat          8119.05     103.78  78.236  &lt; 2e-16 ***\ncolor.L        -132.35     306.56  -0.432 0.666033    \ncolor.Q         315.80     290.51   1.087 0.277286    \ncolor.C         -16.57     275.23  -0.060 0.952018    \ncolor^4         377.23     245.62   1.536 0.124911    \ncolor^5         593.39     239.84   2.474 0.013526 *  \ncolor^6        -225.89     209.92  -1.076 0.282167    \ncarat:color.L -1533.77     313.82  -4.887 1.19e-06 ***\ncarat:color.Q -1137.53     294.94  -3.857 0.000122 ***\ncarat:color.C   110.35     287.00   0.384 0.700695    \ncarat:color^4  -311.72     262.03  -1.190 0.234479    \ncarat:color^5  -865.55     257.40  -3.363 0.000802 ***\ncarat:color^6   255.92     222.64   1.149 0.250641    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1375 on 986 degrees of freedom\nMultiple R-squared:  0.8864,    Adjusted R-squared:  0.8849 \nF-statistic: 591.9 on 13 and 986 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nEach level of the color factor has its own intercept and its own slope for carat.\nThe reference group (e.g., color = “D”) uses the coefficient for carat directly:\n\n\\[ \\text{Slope}_D = \\beta_{carat} \\]\n\nFor other groups, the slope is:\n\n\\[ \\text{Slope}_{\\text{color}} = \\beta_{carat} + \\beta_{\\text{carat:color}} \\]\nExample: If carat has a coefficient of 8000 and carat:colorE has a coefficient of -1000, then for color E diamonds:\n\nIntercept = Intercept + \\(\\beta_{\\text{colorE}}\\)\nSlope = $8000 - 1000 = $7000\n\nSo carat still increases price for color E diamonds, but not as sharply as it does for the reference group (D).\n\n\nVisualizing Interactions\n\nYou can visualize the effect of interaction terms by plotting separate regression lines for each group:\n\n\nCode\nggplot(df, aes(x = carat, y = price, color = color)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n  labs(\n    title = \"Separate Regression Lines by Diamond Color\",\n    x = \"Carat\",\n    y = \"Price (USD)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNote: Each color group has its own slope and intercept, as determined by the interaction model.\n\n\n\n\n\n\nNote\n\n\n\nMultiple linear regression allows us to model relationships between one response variable and many predictors.\nCategorical variables create parallel lines (same slope, different intercepts) unless interaction terms are added.\nInteraction terms create separate lines of best fit (different slopes and intercepts) for each group.\n\n\n\n\n\n\n3.2.3 Goodness of Fit\nWe use several metrics to assess the quality of a regression model:\n\n\\(R^2\\): Proportion of variance in the response explained by the predictors.\nResidual Standard Error (RSE): Average size of the residuals.\nF-statistic: Overall significance of the regression.\nResidual Plots: Visual diagnostics to assess assumptions.\n\n\n\nCode\n# Residual plot\nplot(lm_multi, which = 1)  # Residuals vs Fitted\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Histogram of residuals\nresiduals &lt;- resid(lm_multi)\nhist(residuals, breaks = 30, col = \"lightblue\", main = \"Histogram of Residuals\", xlab = \"Residual\")\n\n\n\n\n\n\n\n\n\nWe want residuals to be roughly normally distributed and randomly scattered around zero to satisfy assumptions of linear regression.\n\n\n3.2.4 ANOVA\nAnalysis of Variance (ANOVA) is a statistical method used to compare the means of three or more groups to determine if at least one of the group means is significantly different from the others.\n\n\n3.2.4.1 Theoretical Foundation\nANOVA works by partitioning the total variability in the data into two components: - Between-group variability: how much the group means differ from the overall mean. - Within-group variability: how much individual observations vary within each group.\nThe core idea is that if the between-group variability is large relative to the within-group variability, then at least one group mean is likely different.\n[ F = = ]\nWhere: - ( SSB ) = Sum of Squares Between - ( SSW ) = Sum of Squares Within - ( k ) = number of groups - ( n ) = total number of observations\nIf the calculated F-statistic is large, and the p-value is small (typically &lt; 0.05), we reject the null hypothesis:\n\n( H_0: _1 = _2 = = _k ) (all group means are equal)\n( H_A: ) At least one group mean is different\n\n\n\n\n3.2.4.2 Example: Do Different Diamond Cuts Have Different Average Prices?\nWe’ll use the diamonds dataset and compare the mean price across the five levels of the cut variable.\n\n\nCode\n# Sample for speed\nset.seed(206)\ndf &lt;- diamonds %&gt;% sample_n(1000)\n\n# Summary statistics by cut\ndf %&gt;%\n  group_by(cut) %&gt;%\n  summarise(mean_price = mean(price), n = n())\n\n\n# A tibble: 5 × 3\n  cut       mean_price     n\n  &lt;ord&gt;          &lt;dbl&gt; &lt;int&gt;\n1 Fair           3677.    28\n2 Good           3429.    74\n3 Very Good      4561.   226\n4 Premium        4355.   273\n5 Ideal          3590.   399\n\n\n\n\n\n3.2.4.3 Run ANOVA Test\n\n\nCode\n# One-way ANOVA: price ~ cut\nanova_model &lt;- aov(price ~ cut, data = df)\nsummary(anova_model)\n\n\n             Df    Sum Sq  Mean Sq F value Pr(&gt;F)  \ncut           4 1.996e+08 49901236   3.065  0.016 *\nResiduals   995 1.620e+10 16283224                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis will output the F-statistic and p-value. A small p-value (e.g. &lt; 0.05) suggests that at least one cut has a significantly different mean price.\n\n\n\n\n3.2.5 Visualize the Group Differences\n\n\nCode\nggplot(df, aes(x = cut, y = price)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"Boxplot of Diamond Price by Cut\",\n       x = \"Cut\", y = \"Price ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBoxplots help visualize both the median and spread of price within each cut level.\n\n\n\n3.2.6 Follow-Up: Which Cuts Are Different?\nIf the ANOVA result is significant, we can follow up with a Tukey HSD test to identify which group pairs differ.\n\n\nCode\nTukeyHSD(anova_model)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = price ~ cut, data = df)\n\n$cut\n                        diff        lwr        upr     p adj\nGood-Fair         -247.90830 -2694.6097 2198.79307 0.9987125\nVery Good-Fair     884.55594 -1324.7679 3093.87977 0.8096253\nPremium-Fair       678.19322 -1510.0655 2866.45192 0.9157755\nIdeal-Fair         -86.59085 -2242.4691 2069.28738 0.9999672\nVery Good-Good    1132.46424  -344.4875 2609.41597 0.2227928\nPremium-Good       926.10152  -519.1496 2371.35262 0.4030816\nIdeal-Good         161.31745 -1234.4209 1557.05582 0.9978434\nPremium-Very Good -206.36272 -1198.0859  785.36051 0.9795621\nIdeal-Very Good   -971.14679 -1889.2153  -53.07827 0.0320166\nIdeal-Premium     -764.78408 -1630.9331  101.36495 0.1126355\n\n\nThis test controls the family-wise error rate and gives pairwise confidence intervals and p-values.\n\n\n\n3.2.7 Interpretation\n\nIf p &lt; 0.05 in the ANOVA, we conclude that at least one cut differs in mean price.\nUse TukeyHSD to find out which cuts are significantly different.\nANOVA assumes:\n\nIndependent observations\nNormally distributed residuals\nEqual variances across groups (can check with Levene’s test)"
  },
  {
    "objectID": "code_annex.html",
    "href": "code_annex.html",
    "title": "R Code Annex",
    "section": "",
    "text": "This page provides a quick reference for the most commonly used R functions in MA206X.\n\n\nlibrary(tidyverse)   # Data manipulation and visualization\nlibrary(kableExtra)  # Enhanced tables\nlibrary(patchwork)   # Combining plots"
  },
  {
    "objectID": "code_annex.html#essential-r-functions-for-statistics",
    "href": "code_annex.html#essential-r-functions-for-statistics",
    "title": "R Code Annex",
    "section": "",
    "text": "This page provides a quick reference for the most commonly used R functions in MA206X.\n\n\nlibrary(tidyverse)   # Data manipulation and visualization\nlibrary(kableExtra)  # Enhanced tables\nlibrary(patchwork)   # Combining plots"
  },
  {
    "objectID": "code_annex.html#data-manipulation-dplyr",
    "href": "code_annex.html#data-manipulation-dplyr",
    "title": "R Code Annex",
    "section": "2 Data Manipulation (dplyr)",
    "text": "2 Data Manipulation (dplyr)\n\n\n\n\n\n\n\n\nFunction\nPurpose\nExample\n\n\n\n\nselect()\nChoose columns\ndf %&gt;% select(name, age)\n\n\nfilter()\nChoose rows\ndf %&gt;% filter(age &gt; 21)\n\n\nmutate()\nCreate/modify columns\ndf %&gt;% mutate(age_squared = age^2)\n\n\narrange()\nSort rows\ndf %&gt;% arrange(desc(score))\n\n\ngroup_by()\nGroup for summaries\ndf %&gt;% group_by(company)\n\n\nsummarize()\nCreate summaries\ndf %&gt;% summarize(mean_age = mean(age))"
  },
  {
    "objectID": "code_annex.html#descriptive-statistics",
    "href": "code_annex.html#descriptive-statistics",
    "title": "R Code Annex",
    "section": "3 Descriptive Statistics",
    "text": "3 Descriptive Statistics\n\n3.1 Measures of Location\nmean(x)              # Arithmetic mean\nmedian(x)            # Median\nquantile(x, 0.25)    # 25th percentile (Q1)\nquantile(x, 0.75)    # 75th percentile (Q3)\n\n\n3.2 Measures of Spread\nvar(x)               # Variance\nsd(x)                # Standard deviation\nIQR(x)               # Interquartile range\nrange(x)             # Min and max\nmax(x) - min(x)      # Range value\n\n\n3.3 Comprehensive Summary\nsummary(x)           # Five-number summary + mean"
  },
  {
    "objectID": "code_annex.html#data-visualization-ggplot2",
    "href": "code_annex.html#data-visualization-ggplot2",
    "title": "R Code Annex",
    "section": "4 Data Visualization (ggplot2)",
    "text": "4 Data Visualization (ggplot2)\n\n4.1 Basic Template\nggplot(data = df, aes(x = variable)) +\n  geom_[type]() +\n  labs(title = \"Title\", x = \"X-axis\", y = \"Y-axis\") +\n  theme_minimal()\n\n\n4.2 Common Plot Types\nHistogram (one quantitative variable):\nggplot(df, aes(x = score)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", color = \"white\")\nBoxplot (one quantitative variable):\nggplot(df, aes(y = score)) +\n  geom_boxplot(fill = \"steelblue\")\nBar Chart (one categorical variable):\nggplot(df, aes(x = category)) +\n  geom_bar(fill = \"steelblue\")\nScatterplot (two quantitative variables):\nggplot(df, aes(x = var1, y = var2)) +\n  geom_point()\nSide-by-side boxplots (categorical + quantitative):\nggplot(df, aes(x = group, y = score, fill = group)) +\n  geom_boxplot()\nFaceted plots (multiple subplots):\nggplot(df, aes(x = var)) +\n  geom_histogram() +\n  facet_wrap(~category)"
  },
  {
    "objectID": "code_annex.html#probability-distributions",
    "href": "code_annex.html#probability-distributions",
    "title": "R Code Annex",
    "section": "5 Probability Distributions",
    "text": "5 Probability Distributions\n\n5.1 Normal Distribution\ndnorm(x, mean, sd)    # Probability density at x\npnorm(q, mean, sd)    # P(X ≤ q)\nqnorm(p, mean, sd)    # Find quantile for probability p\nrnorm(n, mean, sd)    # Generate n random values\n\n\n5.2 Binomial Distribution\ndbinom(x, size, prob)  # P(X = x)\npbinom(q, size, prob)  # P(X ≤ q)\nqbinom(p, size, prob)  # Find quantile\nrbinom(n, size, prob)  # Generate random values\n\n\n5.3 Other Distributions\nReplace norm or binom with:\n\nt for t-distribution\nexp for exponential\npois for Poisson\nchisq for chi-square\nf for F-distribution"
  },
  {
    "objectID": "code_annex.html#hypothesis-testing",
    "href": "code_annex.html#hypothesis-testing",
    "title": "R Code Annex",
    "section": "6 Hypothesis Testing",
    "text": "6 Hypothesis Testing\n\n6.1 One-sample t-test\nt.test(x, mu = 0, alternative = \"two.sided\")\n# alternative: \"two.sided\", \"less\", or \"greater\"\n\n\n6.2 Two-sample t-test\nt.test(x, y, var.equal = FALSE)  # Welch's t-test\nt.test(x, y, var.equal = TRUE)   # Pooled t-test\n\n\n6.3 Paired t-test\nt.test(x, y, paired = TRUE)\n\n\n6.4 Proportion test\nprop.test(x = successes, n = trials, p = 0.5)"
  },
  {
    "objectID": "code_annex.html#confidence-intervals",
    "href": "code_annex.html#confidence-intervals",
    "title": "R Code Annex",
    "section": "7 Confidence Intervals",
    "text": "7 Confidence Intervals\n\n7.1 For means\nt.test(x, conf.level = 0.95)$conf.int\n\n\n7.2 For proportions\nprop.test(x = successes, n = trials)$conf.int"
  },
  {
    "objectID": "code_annex.html#correlation-and-regression",
    "href": "code_annex.html#correlation-and-regression",
    "title": "R Code Annex",
    "section": "8 Correlation and Regression",
    "text": "8 Correlation and Regression\n\n8.1 Correlation\ncor(x, y)                    # Correlation coefficient\ncor.test(x, y)               # Test significance\n\n\n8.2 Linear Regression\nmodel &lt;- lm(y ~ x, data = df)\nsummary(model)               # Model summary\npredict(model, newdata)      # Predictions\n\n\n8.3 Multiple regression\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = df)"
  },
  {
    "objectID": "code_annex.html#anova",
    "href": "code_annex.html#anova",
    "title": "R Code Annex",
    "section": "9 ANOVA",
    "text": "9 ANOVA\nmodel &lt;- aov(response ~ group, data = df)\nsummary(model)\nTukeyHSD(model)              # Post-hoc pairwise comparisons"
  },
  {
    "objectID": "code_annex.html#useful-utilities",
    "href": "code_annex.html#useful-utilities",
    "title": "R Code Annex",
    "section": "10 Useful Utilities",
    "text": "10 Useful Utilities\n\n10.1 Data Creation\nc(1, 2, 3, 4, 5)             # Combine values into vector\nseq(1, 10, by = 2)           # Sequence: 1, 3, 5, 7, 9\nrep(5, times = 3)            # Repeat: 5, 5, 5\n1:10                         # Sequence: 1, 2, ..., 10\n\n\n10.2 Sampling\nsample(x, size = 10, replace = FALSE)  # Random sample\nset.seed(123)                           # Set random seed for reproducibility\n\n\n10.3 Logical Operators\n==    # Equal to\n!=    # Not equal to\n&gt;     # Greater than\n&lt;     # Less than\n&gt;=    # Greater than or equal\n&lt;=    # Less than or equal\n&     # AND\n|     # OR\n!     # NOT\n\n\n10.4 Missing Values\nis.na(x)                     # Check for NA\nna.omit(df)                  # Remove rows with NA\nmean(x, na.rm = TRUE)        # Calculate mean, ignoring NA"
  },
  {
    "objectID": "code_annex.html#tidyverse-workflow-example",
    "href": "code_annex.html#tidyverse-workflow-example",
    "title": "R Code Annex",
    "section": "11 Tidyverse Workflow Example",
    "text": "11 Tidyverse Workflow Example\n# Complete analysis pipeline\nresults &lt;- df %&gt;%\n  filter(age &gt;= 18) %&gt;%           # Keep adults only\n  mutate(bmi = weight/height^2) %&gt;%  # Calculate BMI\n  group_by(gender) %&gt;%             # Group by gender\n  summarize(                       # Calculate summaries\n    n = n(),\n    mean_bmi = mean(bmi),\n    sd_bmi = sd(bmi)\n  ) %&gt;%\n  arrange(desc(mean_bmi))          # Sort by BMI\n\n# View results\nresults"
  },
  {
    "objectID": "code_annex.html#getting-help",
    "href": "code_annex.html#getting-help",
    "title": "R Code Annex",
    "section": "12 Getting Help",
    "text": "12 Getting Help\n?function_name       # Help for specific function\n??search_term        # Search help files\nhelp.start()         # HTML help interface\nexample(mean)        # Run examples for function"
  },
  {
    "objectID": "code_annex.html#keyboard-shortcuts-rstudio",
    "href": "code_annex.html#keyboard-shortcuts-rstudio",
    "title": "R Code Annex",
    "section": "13 Keyboard Shortcuts (RStudio)",
    "text": "13 Keyboard Shortcuts (RStudio)\n\n\n\nAction\nWindows/Linux\nMac\n\n\n\n\nRun current line\nCtrl + Enter\nCmd + Enter\n\n\nAssignment operator &lt;-\nAlt + -\nOption + -\n\n\nPipe operator %&gt;%\nCtrl + Shift + M\nCmd + Shift + M\n\n\nComment/uncomment\nCtrl + Shift + C\nCmd + Shift + C\n\n\nInsert code chunk\nCtrl + Alt + I\nCmd + Option + I"
  },
  {
    "objectID": "code_annex.html#common-error-messages",
    "href": "code_annex.html#common-error-messages",
    "title": "R Code Annex",
    "section": "14 Common Error Messages",
    "text": "14 Common Error Messages\nError in…: object ‘x’ not found - Object doesn’t exist; check spelling or create it first\nError: could not find function… - Function not loaded; load required package with library()\nWarning: NAs introduced by coercion - R couldn’t convert values; check data types\nError in if…: missing value where TRUE/FALSE needed - NA values in logical condition; use na.rm = TRUE or filter NAs first\n\nThis reference sheet covers the most common R operations for MA206X. For more details, see the official R documentation or Tidyverse documentation."
  },
  {
    "objectID": "course_guide/code_annex.html",
    "href": "course_guide/code_annex.html",
    "title": "MA206: Code Annex",
    "section": "",
    "text": "1 Adding Dependencies\nIn RStudio we use a common framework called tidyverse which holds a plethora of useful functions to work with and manipulate data. The other packages are not always necessary, only add the libraries you need at the beginning of your code document. If you do not have that library installed, in the console tab in the bottom-left pane of your RStudio use the function install.packages('the_package_you_want_to_install') and RStudio will install it for you.\n\n\n2 Data Sampling\n\n\nCode\nset.seed(1991)\ndata(diamonds)\ndf &lt;- diamonds %&gt;% sample_n(size = 1000)\n\n\n\n\n3 Table Preview\n\n\nCode\ndf %&gt;% head() %&gt;%\n  kable(\"html\", align = \"c\") %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    position = \"center\",\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"bordered\")\n  )\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n1.02\nGood\nG\nVS2\n63.8\n59.0\n6080\n6.34\n6.27\n4.02\n\n\n0.31\nIdeal\nF\nVVS1\n61.9\n53.5\n882\n4.36\n4.39\n2.71\n\n\n0.60\nPremium\nD\nSI2\n61.3\n61.0\n1428\n5.46\n5.40\n3.33\n\n\n0.41\nIdeal\nE\nIF\n62.1\n54.0\n1419\n4.75\n4.81\n2.97\n\n\n0.72\nVery Good\nH\nVS1\n62.2\n54.0\n2877\n5.74\n5.76\n3.57\n\n\n1.20\nIdeal\nF\nVS2\n62.6\n56.0\n8486\n6.78\n6.73\n4.23\n\n\n\n\n\n\n\n4 Histograms\n\n\nCode\np1 &lt;- df %&gt;% ggplot(aes(x = carat)) + geom_histogram(bins = 30) + theme_minimal()\np2 &lt;- df %&gt;% ggplot(aes(x = depth)) + geom_histogram(bins = 30) + theme_minimal()\np3 &lt;- df %&gt;% ggplot(aes(x = price)) + geom_histogram(bins = 30) + theme_minimal()\np1 | p2 | p3\n\n\n\n\n\n\n\n\n\n\n\n5 Boxplots\n\n\nCode\np4 &lt;- df %&gt;% ggplot(aes(x = carat)) + geom_boxplot() + theme_minimal()\np5 &lt;- df %&gt;% ggplot(aes(x = depth)) + geom_boxplot() + theme_minimal()\np6 &lt;- df %&gt;% ggplot(aes(x = price)) + geom_boxplot() + theme_minimal()\np4 | p5 | p6\n\n\n\n\n\n\n\n\n\n\n\n6 Summary Statistics Table\n\n\nCode\ndf %&gt;%\n  summarise(\n    across(\n      where(is.numeric),\n      list(\n        Mean = ~mean(.x, na.rm = TRUE),\n        Median = ~median(.x, na.rm = TRUE),\n        SD = ~sd(.x, na.rm = TRUE),\n        Var = ~var(.x, na.rm = TRUE),\n        Min = ~min(.x, na.rm = TRUE),\n        Max = ~max(.x, na.rm = TRUE)\n      ),\n      .names = \"{.col}_{.fn}\"\n    )\n  ) %&gt;%\n  round(2) %&gt;%\n  pivot_longer(everything(), names_to = c(\"Variable\", \"Stat\"), names_sep = \"_\") %&gt;%\n  pivot_wider(names_from = Stat, values_from = value) %&gt;%\n  kable(\"html\", align = \"c\") %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    position = \"center\",\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"bordered\")\n  )\n\n\n\n\n\nVariable\nMean\nMedian\nSD\nVar\nMin\nMax\n\n\n\n\ncarat\n0.79\n0.70\n0.46\n0.21\n0.23\n3.24\n\n\ndepth\n61.77\n61.90\n1.40\n1.97\n56.30\n68.90\n\n\ntable\n57.48\n57.00\n2.26\n5.11\n50.00\n66.00\n\n\nprice\n3873.55\n2387.00\n3912.55\n15308046.06\n337.00\n18692.00\n\n\nx\n5.72\n5.68\n1.10\n1.21\n3.88\n9.44\n\n\ny\n5.72\n5.68\n1.09\n1.20\n3.90\n9.40\n\n\nz\n3.53\n3.52\n0.68\n0.46\n2.39\n5.85\n\n\n\n\n\n\n\n7 Scatterplots\n\n\nCode\np7 &lt;- df %&gt;% ggplot(aes(x = carat, y = price, color = clarity)) + geom_point() + theme_minimal()\np8 &lt;- df %&gt;% ggplot(aes(x = table, y = price, color = cut)) + geom_point() + theme_minimal()\np9 &lt;- df %&gt;% ggplot(aes(x = depth, y = price, color = color)) + geom_point() + theme_minimal()\np7 | p8 | p9\n\n\n\n\n\n\n\n\n\n\n\n8 Bar Charts\n\n\nCode\np10 &lt;- df %&gt;% group_by(cut) %&gt;% summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = cut, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"pink\") +\n  labs(title = \"Average Price by Diamond Cut\", x = \"Cut\", y = \"Avg Price\") +\n  theme_minimal()\n\np11 &lt;- df %&gt;% group_by(color) %&gt;% summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = color, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Average Price by Diamond Color\", x = \"Color\", y = \"Avg Price\") +\n  theme_minimal()\n\np12 &lt;- df %&gt;% group_by(clarity) %&gt;% summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = clarity, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"magenta\") +\n  labs(title = \"Average Price by Diamond Clarity\", x = \"Clarity\", y = \"Avg Price\") +\n  theme_minimal()\n\np10 | p11 | p12\n\n\n\n\n\n\n\n\n\n\n\n9 Correlation Table\n\n\nCode\ndf %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor(use = \"pairwise.complete.obs\") %&gt;%\n  round(2) %&gt;%\n  kable(\"html\", align = \"c\") %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    position = \"center\",\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"bordered\")\n  )\n\n\n\n\n\n\ncarat\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\ncarat\n1.00\n0.00\n0.22\n0.91\n0.98\n0.98\n0.98\n\n\ndepth\n0.00\n1.00\n-0.32\n-0.02\n-0.06\n-0.07\n0.06\n\n\ntable\n0.22\n-0.32\n1.00\n0.17\n0.24\n0.24\n0.20\n\n\nprice\n0.91\n-0.02\n0.17\n1.00\n0.88\n0.88\n0.88\n\n\nx\n0.98\n-0.06\n0.24\n0.88\n1.00\n1.00\n0.99\n\n\ny\n0.98\n-0.07\n0.24\n0.88\n1.00\n1.00\n0.99\n\n\nz\n0.98\n0.06\n0.20\n0.88\n0.99\n0.99\n1.00\n\n\n\n\n\n\n\n10 Proportion Test Example\n\n\nCode\npi &lt;- 0.4\nn &lt;- nrow(diamonds)\nphat &lt;- mean(diamonds$cut == \"Ideal\")\nz &lt;- (phat - pi) / sqrt(pi * (1 - pi) / n)\np_value &lt;- 1 - pnorm(z)\ncat(\"Z-statistic: \", round(z, 3), \"\\nP-value: \", round(p_value, 4))\n\n\nZ-statistic:  -0.22 \nP-value:  0.587\n\n\n\n\n11 Mean Test Example\n\n\nCode\nmu0 &lt;- 4000\nx_bar &lt;- mean(diamonds$price)\ns &lt;- sd(diamonds$price)\nn &lt;- length(diamonds$price)\nt_stat &lt;- (x_bar - mu0) / (s / sqrt(n))\np_value &lt;- 2 * (1 - pt(abs(t_stat), df = n - 1))\ncat(\"T-statistic: \", round(t_stat, 3), \"\\nP-value: \", round(p_value, 4))\n\n\nT-statistic:  -3.912 \nP-value:  1e-04\n\n\n\n\n12 Two-Sample Proportion Test\n\n\nCode\ndf_prop &lt;- diamonds %&gt;% filter(color %in% c(\"D\", \"E\")) %&gt;% mutate(is_ideal = cut == \"Ideal\")\np1 &lt;- mean(df_prop$is_ideal[df_prop$color == \"D\"])\np2 &lt;- mean(df_prop$is_ideal[df_prop$color == \"E\"])\nn1 &lt;- sum(df_prop$color == \"D\")\nn2 &lt;- sum(df_prop$color == \"E\")\nphat_pool &lt;- (p1 * n1 + p2 * n2) / (n1 + n2)\nz &lt;- (p1 - p2) / sqrt(phat_pool * (1 - phat_pool) * (1 / n1 + 1 / n2))\np_value &lt;- 2 * (1 - pnorm(abs(z)))\ncat(\"Z-statistic: \", round(z, 3), \"\\nP-value: \", round(p_value, 4))\n\n\nZ-statistic:  2.566 \nP-value:  0.0103\n\n\n\n\n13 Chi-Square Test\n\n\nCode\ntbl &lt;- table(diamonds$cut, diamonds$color)\nchisq.test(tbl)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tbl\nX-squared = 310.32, df = 24, p-value &lt; 2.2e-16\n\n\n\n\n14 Independent T-Test\n\n\nCode\ndf_t &lt;- diamonds %&gt;% filter(cut %in% c(\"Ideal\", \"Fair\"))\nt.test(price ~ cut, data = df_t)\n\n\n\n    Welch Two Sample t-test\n\ndata:  price by cut\nt = 9.7484, df = 1894.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Fair and group Ideal is not equal to 0\n95 percent confidence interval:\n  719.9065 1082.5251\nsample estimates:\n mean in group Fair mean in group Ideal \n           4358.758            3457.542 \n\n\n\n\n15 Paired T-Test (Simulated)\n\n\nCode\nset.seed(123)\nn &lt;- 100\norig &lt;- sample(diamonds$price, n)\ndisc &lt;- orig * runif(n, 0.85, 0.95)\nt.test(orig, disc, paired = TRUE)\n\n\n\n    Paired t-test\n\ndata:  orig and disc\nt = 8.9496, df = 99, p-value = 2.132e-14\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 342.2012 537.1645\nsample estimates:\nmean difference \n       439.6829 \n\n\n\n\n16 Simple Linear Regression\n\n\nCode\nset.seed(206)\ndf_lm &lt;- diamonds %&gt;% sample_n(1000)\nlm_simple &lt;- lm(price ~ carat, data = df_lm)\nsummary(lm_simple)\n\n\n\nCall:\nlm(formula = price ~ carat, data = df_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8077.3  -813.1    10.2   607.5  8759.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2376.64      93.40  -25.45   &lt;2e-16 ***\ncarat        7885.65      99.37   79.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1499 on 998 degrees of freedom\nMultiple R-squared:  0.8632,    Adjusted R-squared:  0.8631 \nF-statistic:  6297 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n17 Multiple Linear Regression\n\n\nCode\nlm_multi &lt;- lm(price ~ carat + depth + table, data = df_lm)\nsummary(lm_multi)\n\n\n\nCall:\nlm(formula = price ~ carat + depth + table, data = df_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8252.9  -784.5   -22.4   624.7  8211.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13392.24    2845.47   4.707 2.88e-06 ***\ncarat        7965.09      99.01  80.445  &lt; 2e-16 ***\ndepth        -146.58      35.32  -4.149 3.62e-05 ***\ntable        -118.01      22.05  -5.351 1.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1475 on 996 degrees of freedom\nMultiple R-squared:  0.8679,    Adjusted R-squared:  0.8675 \nF-statistic:  2181 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n18 Regression Diagnostics\n\n\nCode\nplot(lm_multi, which = 1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nresid &lt;- resid(lm_multi)\nhist(resid, breaks = 30, main = \"Histogram of Residuals\", xlab = \"Residual\")\n\n\n\n\n\n\n\n\n\n\n\n19 ANOVA\n\n\nCode\nanova_model &lt;- aov(price ~ cut, data = df_lm)\nsummary(anova_model)\n\n\n             Df    Sum Sq  Mean Sq F value Pr(&gt;F)  \ncut           4 1.996e+08 49901236   3.065  0.016 *\nResiduals   995 1.620e+10 16283224                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n20 Tukey HSD\nTukeyHSD(anova_model)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA206X: Probability and Statistics",
    "section": "",
    "text": "This website contains interactive lesson materials for MA206X: Introduction to Probability and Statistics, following Devore’s 9th Edition of Probability and Statistics for Engineering and the Sciences.\n\n\nThis course introduces you to the foundational principles of probability and statistics, emphasizing data literacy and inference. The course is organized into three blocks:\n\n\n\nUnderstanding data types and study design\nExploratory data analysis and visualization\nProbability theory and counting principles\nDiscrete and continuous probability distributions\n\n\n\n\n\nCentral Limit Theorem and sampling distributions\nConfidence intervals for means and proportions\nHypothesis testing (one-sample and two-sample)\nType I/II errors and statistical power\n\n\n\n\n\nSimple and multiple linear regression\nANOVA and experimental design\nModel diagnostics and interpretation\nReal-world applications with Army data\n\n\n\n\n\nEvery statistical analysis in this course follows a clear, repeatable process:\n\nAsk a research question - Formulate a meaningful, testable question\nDesign a study and collect data - Choose appropriate sampling methods\nExplore the data - Use visualizations and summary statistics\nDraw inferences beyond the data - Apply statistical tests and models\nFormulate conclusions - Interpret results in context\nLook back and ahead - Reflect on limitations and future directions\n\n\n\n\nThroughout this course, we’ll use R for data analysis. The primary focus is not on becoming coding experts, but on using R as a tool to:\n\nSummarize and visualize data effectively\nPerform statistical tests and create models\nCommunicate findings clearly and reproducibly\n\nBuilding on skills from CY105, you’ll develop practical data analysis capabilities that support your understanding of statistical concepts.\n\n\n\nNavigate to the lessons using the menu above. Each lesson includes:\n\nLearning objectives aligned with the syllabus\nConceptual explanations with mathematical foundations\nR code examples you can run and modify\nPractice problems to reinforce understanding\n\n\n\n\n\n\n\nUsing This Site\n\n\n\n\nCode blocks can be copied using the copy button in the top-right\nClick “Code” buttons to show/hide R code\nUse the table of contents on the right to navigate within lessons\nTry modifying and running the R examples yourself!\n\n\n\n\n\n\n\nCode Annex - Quick reference for R functions\nCourse Syllabus - Full syllabus with all 40 lessons\n\n\nThis website is designed to replace traditional lesson handouts, allowing you to follow along with boardwork, experiment with R code, and revisit materials anytime."
  },
  {
    "objectID": "index.html#welcome-to-ma206x",
    "href": "index.html#welcome-to-ma206x",
    "title": "MA206X: Probability and Statistics",
    "section": "",
    "text": "This website contains interactive lesson materials for MA206X: Introduction to Probability and Statistics, following Devore’s 9th Edition of Probability and Statistics for Engineering and the Sciences.\n\n\nThis course introduces you to the foundational principles of probability and statistics, emphasizing data literacy and inference. The course is organized into three blocks:\n\n\n\nUnderstanding data types and study design\nExploratory data analysis and visualization\nProbability theory and counting principles\nDiscrete and continuous probability distributions\n\n\n\n\n\nCentral Limit Theorem and sampling distributions\nConfidence intervals for means and proportions\nHypothesis testing (one-sample and two-sample)\nType I/II errors and statistical power\n\n\n\n\n\nSimple and multiple linear regression\nANOVA and experimental design\nModel diagnostics and interpretation\nReal-world applications with Army data\n\n\n\n\n\nEvery statistical analysis in this course follows a clear, repeatable process:\n\nAsk a research question - Formulate a meaningful, testable question\nDesign a study and collect data - Choose appropriate sampling methods\nExplore the data - Use visualizations and summary statistics\nDraw inferences beyond the data - Apply statistical tests and models\nFormulate conclusions - Interpret results in context\nLook back and ahead - Reflect on limitations and future directions\n\n\n\n\nThroughout this course, we’ll use R for data analysis. The primary focus is not on becoming coding experts, but on using R as a tool to:\n\nSummarize and visualize data effectively\nPerform statistical tests and create models\nCommunicate findings clearly and reproducibly\n\nBuilding on skills from CY105, you’ll develop practical data analysis capabilities that support your understanding of statistical concepts.\n\n\n\nNavigate to the lessons using the menu above. Each lesson includes:\n\nLearning objectives aligned with the syllabus\nConceptual explanations with mathematical foundations\nR code examples you can run and modify\nPractice problems to reinforce understanding\n\n\n\n\n\n\n\nUsing This Site\n\n\n\n\nCode blocks can be copied using the copy button in the top-right\nClick “Code” buttons to show/hide R code\nUse the table of contents on the right to navigate within lessons\nTry modifying and running the R examples yourself!\n\n\n\n\n\n\n\nCode Annex - Quick reference for R functions\nCourse Syllabus - Full syllabus with all 40 lessons\n\n\nThis website is designed to replace traditional lesson handouts, allowing you to follow along with boardwork, experiment with R code, and revisit materials anytime."
  },
  {
    "objectID": "lesson_02.html",
    "href": "lesson_02.html",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "",
    "text": "By the end of this lesson, you will be able to:\n\nContrast observational studies and designed experiments\nDescribe common sampling methods and potential biases\nExplain why randomization and control strengthen inference\n\nReading: Devore 1.2"
  },
  {
    "objectID": "lesson_02.html#introduction",
    "href": "lesson_02.html#introduction",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn Lesson 1, we learned to distinguish populations from samples. Now we address the critical question: How do we collect data that leads to valid conclusions?\nThe way we collect data fundamentally determines:\n\nWhat questions we can answer\nHow confident we can be in our conclusions\nWhether we can establish cause-and-effect relationships"
  },
  {
    "objectID": "lesson_02.html#observational-studies-vs.-designed-experiments",
    "href": "lesson_02.html#observational-studies-vs.-designed-experiments",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "2 Observational Studies vs. Designed Experiments",
    "text": "2 Observational Studies vs. Designed Experiments\n\n2.1 Observational Studies\n\n\n\n\n\n\nDefinition: Observational Study\n\n\n\nAn observational study observes individuals and measures variables of interest without attempting to influence the responses.\nKey characteristic: The researcher does not assign treatments or interventions.\n\n\nStrengths:\n\nCan study factors that would be unethical to manipulate\nOften less expensive and faster\nCan observe real-world behavior\n\nLimitations:\n\nCannot establish causation (only association)\nConfounding variables may influence results\nSubject to various biases\n\n\n\n2.2 Example 2.1: Observational Study\nResearch Question: Do cadets who eat breakfast perform better academically?\nStudy Design: Survey 500 cadets about their breakfast habits and obtain their GPAs from the registrar.\nFinding: Cadets who regularly eat breakfast have a 0.3 higher average GPA.\nCan we conclude breakfast causes higher GPA?\nNo! Other factors (confounders) might explain this:\n\nMore disciplined cadets both eat breakfast AND study more\nAthletes (who tend to eat breakfast) may have different academic profiles\nMorning classes might influence both breakfast habits and performance\n\n\n\n2.3 Designed Experiments\n\n\n\n\n\n\nDefinition: Designed Experiment\n\n\n\nA designed experiment deliberately imposes treatments on individuals and observes their responses.\nKey characteristics:\n\nResearcher assigns treatments\nIncludes randomization\nOften includes a control group\n\n\n\nStrengths:\n\nCan establish cause-and-effect relationships\nControl for confounding variables\nAllows for replication\n\nLimitations:\n\nMay be expensive or time-consuming\nSome treatments are unethical to assign\nArtificial settings may not reflect real-world conditions\n\n\n\n2.4 Example 2.2: Designed Experiment\nResearch Question: Does a new training program improve ruck march performance?\nExperimental Design:\n\nRandomly assign 100 soldiers to two groups:\n\nTreatment group (n=50): New training program\nControl group (n=50): Standard training program\n\nAfter 8 weeks, measure 12-mile ruck march times for all soldiers\nCompare average times between groups\n\nWhy randomization matters: Random assignment ensures that other factors (fitness level, age, experience) are balanced between groups on average. Any difference in performance can be attributed to the training program.\n\n\n2.5 Key Comparison\n\n\n\n\n\nAspect\nObservational Study\nDesigned Experiment\n\n\n\n\nTreatment Assignment\nResearcher observes naturally occurring groups\nResearcher randomly assigns treatments\n\n\nCausation\nCannot establish\nCan establish (if well-designed)\n\n\nConfounding\nDifficult to control\nControlled through randomization\n\n\nTypical Goal\nDescribe associations\nEstablish cause-effect\n\n\nExample\nSurveying soldiers about sleep and APFT scores\nRandomly assigning soldiers to different sleep schedules"
  },
  {
    "objectID": "lesson_02.html#sampling-methods",
    "href": "lesson_02.html#sampling-methods",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "3 Sampling Methods",
    "text": "3 Sampling Methods\nWhen we cannot study an entire population, we need a sampling method that produces a representative sample.\n\n3.1 Simple Random Sampling (SRS)\n\n\n\n\n\n\nSimple Random Sample\n\n\n\nA simple random sample of size \\(n\\) consists of \\(n\\) individuals from the population chosen in such a way that every set of \\(n\\) individuals has an equal chance of being selected.\nKey property: Every individual has an equal probability of selection.\n\n\nHow to obtain an SRS:\n\nCreate a sampling frame: complete list of all individuals in the population\nAssign each individual a unique number\nUse a random number generator to select \\(n\\) numbers\nSample the individuals corresponding to those numbers\n\n\n\n3.2 Example 2.3: Simple Random Sampling in R\nLet’s simulate selecting a random sample of 10 cadets from a company of 120:\n\n\nCode\n# Set seed for reproducibility\nset.seed(206)\n\n# Create sampling frame: C-numbers for 120 cadets\npopulation &lt;- sprintf(\"C%05d\", 90001:90120)\n\n# Draw a simple random sample of size 10\nsample_srs &lt;- sample(population, size = 10, replace = FALSE)\n\n# Display the selected cadets\nsample_srs\n\n\n [1] \"C90047\" \"C90034\" \"C90085\" \"C90050\" \"C90063\" \"C90084\" \"C90058\" \"C90033\"\n [9] \"C90008\" \"C90100\"\n\n\nEvery cadet had a \\(\\frac{10}{120} = 0.0833\\) (8.33%) chance of being selected.\n\n\n3.3 Stratified Random Sampling\n\n\n\n\n\n\nStratified Random Sample\n\n\n\nStratified sampling divides the population into homogeneous groups (strata), then takes an SRS from each stratum.\nWhen to use: When you want to ensure representation from important subgroups.\n\n\nAdvantages:\n\nGuarantees representation from each stratum\nCan reduce sampling variability\nAllows separate analysis by stratum\n\n\n\n3.4 Example 2.4: Stratified Sampling\nGoal: Survey cadets about online learning preferences, ensuring each class is represented.\nPopulation: 4,400 cadets (1,100 per class)\nStratified Design:\n\nStratum 1: Firsties (1,100 cadets) → Sample 50\nStratum 2: Cows (1,100 cadets) → Sample 50\nStratum 3: Yuks (1,100 cadets) → Sample 50\nStratum 4: Plebes (1,100 cadets) → Sample 50\n\nTotal sample size: 200 cadets\n\n\nCode\nset.seed(206)\n\n# Create population with class labels\nclass_year &lt;- rep(c(\"Firstie\", \"Cow\", \"Yuk\", \"Plebe\"), each = 1100)\ncadet_id &lt;- sprintf(\"C%05d\", 1:4400)\n\npopulation_df &lt;- tibble(cadet_id, class_year)\n\n# Stratified sampling: 50 from each class\nstratified_sample &lt;- population_df %&gt;%\n  group_by(class_year) %&gt;%\n  slice_sample(n = 50) %&gt;%\n  ungroup()\n\n# Verify sample sizes per stratum\nstratified_sample %&gt;%\n  count(class_year) %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nclass_year\nn\n\n\n\n\nCow\n50\n\n\nFirstie\n50\n\n\nPlebe\n50\n\n\nYuk\n50\n\n\n\n\n\n\n\n3.5 Convenience Sampling\n\n\n\n\n\n\nConvenience Sample\n\n\n\nA convenience sample consists of individuals who are easily accessible to the researcher.\nMajor problem: This is NOT a probability-based sampling method and often leads to biased results.\n\n\n\n\n3.6 Example 2.5: Convenience Sampling Bias\nScenario: You want to estimate average cadet satisfaction with dining facilities.\nConvenience sample: Stand in Washington Hall at dinner and survey the first 100 cadets you see.\nProblems:\n\nOnly captures cadets who eat dinner in Washington Hall\nMisses cadets who avoid DFAC due to dissatisfaction\nTime of day affects who you encounter\nMay overrepresent certain companies or teams\n\nBetter approach: SRS or stratified sample from complete cadet roster\n\n\n3.7 Comparison of Sampling Methods\n\n\n\n\n\nMethod\nDescription\nPros\nCons\nGeneralizable?\n\n\n\n\nSimple Random\nEvery individual has equal probability\nUnbiased, straightforward\nMay miss subgroups by chance\nYes\n\n\nStratified Random\nRandom sample from each subgroup\nEnsures subgroup representation\nRequires knowledge of strata\nYes\n\n\nCluster\nRandomly select groups, sample all within\nCost-effective for dispersed populations\nHigher variability within clusters\nYes\n\n\nSystematic\nSelect every kth individual from a list\nSimple to implement\nCan introduce bias if pattern in list\nUsually\n\n\nConvenience\nSelect easily accessible individuals\nQuick and cheap\nHighly biased, non-representative\nNo"
  },
  {
    "objectID": "lesson_02.html#common-sources-of-bias",
    "href": "lesson_02.html#common-sources-of-bias",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "4 Common Sources of Bias",
    "text": "4 Common Sources of Bias\nEven with good sampling methods, bias can creep into studies. Understanding these threats helps us design better studies and critically evaluate research.\n\n4.1 Selection Bias\n\n\n\n\n\n\nSelection Bias\n\n\n\nSelection bias occurs when the method of selecting subjects causes the sample to differ systematically from the population.\n\n\nExamples:\n\nOnline surveys (exclude those without internet access)\nVolunteer samples (self-selected individuals may differ)\nSurvivor bias (only studying survivors of a process)\n\n\n\n4.2 Response Bias\n\n\n\n\n\n\nResponse Bias\n\n\n\nResponse bias occurs when the method of data collection influences the responses given.\n\n\nCommon types:\n\nSocial desirability bias: Respondents give socially acceptable answers rather than truthful ones\nExample: Cadets over-reporting study hours to appear diligent\nQuestion wording effects: How a question is phrased influences responses\nExample: “Do you support protecting freedom?” vs. “Do you support restricting speech?”\nInterviewer effects: Characteristics of the interviewer influence responses\n\n\n\n4.3 Nonresponse Bias\n\n\n\n\n\n\nNonresponse Bias\n\n\n\nNonresponse bias occurs when individuals selected for the sample do not respond, and non-responders differ systematically from responders.\n\n\nExample: Email survey with 30% response rate. If those who don’t respond have different views than those who do, results will be biased.\nMitigation strategies:\n\nFollow-up with non-responders\nOffer incentives\nMake surveys brief and convenient\nCompare responders to known population characteristics"
  },
  {
    "objectID": "lesson_02.html#principles-of-experimental-design",
    "href": "lesson_02.html#principles-of-experimental-design",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "5 Principles of Experimental Design",
    "text": "5 Principles of Experimental Design\nWell-designed experiments incorporate three key principles:\n\n5.1 1. Control\n\n\n\n\n\n\nControl\n\n\n\nControl means keeping all variables except the treatment the same across groups.\nHelps isolate the effect of the treatment from other factors.\n\n\nMethods:\n\nControl group: Group receiving standard treatment or placebo\nControl variables: Measure and account for confounding variables\nMatched pairs: Create pairs of similar subjects, assign different treatments\n\n\n\n5.2 2. Randomization\n\n\n\n\n\n\nRandomization\n\n\n\nRandomization means using chance to assign subjects to treatment groups.\nPurpose: Balance out confounding variables across groups on average.\n\n\nWhy it matters:\n\n\nCode\nset.seed(42)\n\n# Suppose we have 100 soldiers with varying fitness levels\nsoldiers &lt;- tibble(\n  id = 1:100,\n  fitness_score = rnorm(100, mean = 250, sd = 30)\n)\n\n# Random assignment to treatment/control\nsoldiers &lt;- soldiers %&gt;%\n  mutate(group = sample(rep(c(\"Treatment\", \"Control\"), each = 50)))\n\n# Check that randomization balanced fitness scores\nsoldiers %&gt;%\n  group_by(group) %&gt;%\n  summarize(\n    n = n(),\n    mean_fitness = round(mean(fitness_score), 1),\n    sd_fitness = round(sd(fitness_score), 1)\n  ) %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\ngroup\nn\nmean_fitness\nsd_fitness\n\n\n\n\nControl\n50\n251.5\n29.3\n\n\nTreatment\n50\n250.5\n33.3\n\n\n\n\n\nThe groups have similar average fitness despite random assignment!\n\n\n5.3 3. Replication\n\n\n\n\n\n\nReplication\n\n\n\nReplication means:\n\nApplying each treatment to multiple subjects\nRepeating the entire study to verify findings\n\n\n\nWhy it matters:\n\nLarger samples reduce the role of chance variation\nRepeated studies build confidence in conclusions\nAllows assessment of generalizability"
  },
  {
    "objectID": "lesson_02.html#designing-a-study-putting-it-all-together",
    "href": "lesson_02.html#designing-a-study-putting-it-all-together",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "6 Designing a Study: Putting It All Together",
    "text": "6 Designing a Study: Putting It All Together\n\n6.1 Example 2.6: Designing an Experiment\nResearch Question: Does a new marksmanship training app improve rifle qualification scores?\nBad Design (Observational):\n\nLet soldiers choose whether to use the app\nCompare qualification scores between users and non-users\nProblem: Self-selection! Motivated soldiers might both use app AND practice more\n\nGood Design (Experimental):\n\nSubjects: 200 soldiers from the same battalion\nRandomization: Randomly assign to two groups:\n\nTreatment (n=100): Use app for 4 weeks\nControl (n=100): Standard practice for 4 weeks\n\nControl:\n\nSame total practice time for both groups\nSame rifles and ammunition\nSame qualification course and graders\nSame time of year\n\nResponse variable: Qualification score (Marksman/Sharpshooter/Expert)\nAnalysis: Compare score distributions between groups\n\nWhy this design is better:\n\nRandom assignment balances confounders (motivation, prior skill, etc.)\nControl group allows comparison\nSufficient replication (100 per group)\nCan establish causation if difference is found"
  },
  {
    "objectID": "lesson_02.html#simulation-the-power-of-random-sampling",
    "href": "lesson_02.html#simulation-the-power-of-random-sampling",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "7 Simulation: The Power of Random Sampling",
    "text": "7 Simulation: The Power of Random Sampling\nLet’s demonstrate why random sampling works:\n\n\nCode\nset.seed(206)\n\n# Create a population of 10,000 soldiers with a known mean ACFT score\npopulation_acft &lt;- tibble(\n  soldier_id = 1:10000,\n  acft_score = rnorm(10000, mean = 480, sd = 60)\n)\n\n# True population mean\ntrue_mean &lt;- mean(population_acft$acft_score)\n\n# Take 1000 different random samples of size 100\nsampling_distribution &lt;- tibble(\n  sample_number = 1:1000,\n  sample_mean = map_dbl(1:1000, ~mean(sample(population_acft$acft_score, 100)))\n)\n\n# Plot the distribution of sample means\nggplot(sampling_distribution, aes(x = sample_mean)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\") +\n  geom_vline(xintercept = true_mean, color = \"red\", linewidth = 1.5, linetype = \"dashed\") +\n  labs(\n    title = \"Distribution of 1,000 Sample Means (n=100)\",\n    subtitle = paste(\"True population mean =\", round(true_mean, 1)),\n    x = \"Sample Mean ACFT Score\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nKey insight: Most sample means cluster around the true population mean! This is why random sampling works."
  },
  {
    "objectID": "lesson_02.html#practice-problems",
    "href": "lesson_02.html#practice-problems",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "8 Practice Problems",
    "text": "8 Practice Problems\n\n8.1 Practice 2.1\nIdentify whether each study is observational or experimental. If experimental, identify the treatment and control groups.\n\nResearchers compare injury rates between soldiers who wear different boot brands.\nA study randomly assigns half of a company to a new PT program and compares ACFT scores after 6 weeks.\nMedical researchers examine the relationship between sleep duration and reaction time by surveying soldiers.\nCadets are randomly assigned to study either in groups or individually, then take the same exam.\n\n\n\n8.2 Practice 2.2\nFor each sampling scenario, identify:\n\nThe sampling method used\nPotential sources of bias\nWhether results can be generalized to the population\n\n\nTo assess cadet satisfaction, researchers email a survey to all 4,400 cadets and analyze the 800 responses received.\nTo estimate average ruck march time, a researcher randomly selects 5 companies, then times all soldiers in those companies.\nTo study academic performance, researchers randomly select 50 cadets from each of the four classes.\n\n\n\n8.3 Practice 2.3\nDesign an experiment to answer this question: “Does listening to music during study sessions improve exam performance?”\nInclude in your design:\n\nSubjects and sample size\nTreatment and control groups\nHow randomization will be implemented\nWhat variables will be controlled\nHow you will measure the response"
  },
  {
    "objectID": "lesson_02.html#summary",
    "href": "lesson_02.html#summary",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "9 Summary",
    "text": "9 Summary\nKey Takeaways:\n\nObservational studies observe naturally occurring differences; experiments impose treatments\nRandom sampling ensures every individual has a known probability of selection, enabling generalization\nCommon sampling methods:\n\nSimple random sampling (SRS)\nStratified sampling\nConvenience sampling (avoid when possible!)\n\nSources of bias to watch for:\n\nSelection bias\nResponse bias\nNonresponse bias\n\nGood experimental design requires:\n\nControl (isolate treatment effects)\nRandomization (balance confounders)\nReplication (reduce chance variation)\n\n\nIn Lesson 3, we’ll learn how to describe and summarize the data we collect using measures of location."
  },
  {
    "objectID": "lesson_02.html#additional-resources",
    "href": "lesson_02.html#additional-resources",
    "title": "Lesson 2: Sampling & Study Design",
    "section": "10 Additional Resources",
    "text": "10 Additional Resources\n\nDevore Section 1.2\nUnderstanding Statistical Power\nWebAssign practice problems on sampling and study design"
  },
  {
    "objectID": "lesson_04.html",
    "href": "lesson_04.html",
    "title": "Lesson 4: Measures of Variability",
    "section": "",
    "text": "By the end of this lesson, you will be able to:\n\nCalculate and interpret range and inter-quartile range (IQR)\nCalculate and interpret variance and standard deviation\nUse measures of spread to compare distributions\n\nReading: Devore 1.4"
  },
  {
    "objectID": "lesson_04.html#introduction",
    "href": "lesson_04.html#introduction",
    "title": "Lesson 4: Measures of Variability",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn Lesson 3, we learned to describe the center of a distribution using mean, median, and mode. But knowing the center alone doesn’t tell the whole story.\nConsider two platoons with the same average ACFT score of 480:\n\nPlatoon A: All soldiers score between 470-490 (very consistent)\nPlatoon B: Scores range from 360-600 (highly variable)\n\nThese platoons are fundamentally different, yet they have the same mean! We need measures of variability (or spread) to capture this difference."
  },
  {
    "objectID": "lesson_04.html#why-variability-matters",
    "href": "lesson_04.html#why-variability-matters",
    "title": "Lesson 4: Measures of Variability",
    "section": "2 Why Variability Matters",
    "text": "2 Why Variability Matters\n\n\n\n\n\n\nVariability Affects Decision-Making\n\n\n\nLow variability → Predictable, consistent performance\nHigh variability → Unpredictable, inconsistent performance\nBoth have implications for:\n\nResource allocation\nRisk assessment\nQuality control\nTraining effectiveness"
  },
  {
    "objectID": "lesson_04.html#the-range",
    "href": "lesson_04.html#the-range",
    "title": "Lesson 4: Measures of Variability",
    "section": "3 The Range",
    "text": "3 The Range\n\n\n\n\n\n\nDefinition: Range\n\n\n\nThe range is the difference between the maximum and minimum values:\n\\[\\text{Range} = \\max(x) - \\min(x)\\]\n\n\n\n3.1 Advantages and Limitations\nAdvantages:\n\nSimple to calculate\nEasy to interpret\nGives quick sense of spread\n\nLimitations:\n\nUses only two values (ignores all others)\nExtremely sensitive to outliers\nIncreases with sample size\n\n\n\n3.2 Example 4.1: Range\nPush-up counts: 45, 50, 55, 60, 65\n\\[\\text{Range} = 65 - 45 = 20 \\text{ push-ups}\\]\n\n\nCode\npushups &lt;- c(45, 50, 55, 60, 65)\nrange_val &lt;- max(pushups) - min(pushups)\nrange_val\n\n\n[1] 20\n\n\nBut if one soldier does 100 push-ups:\n\n\nCode\npushups_outlier &lt;- c(45, 50, 55, 60, 100)\nmax(pushups_outlier) - min(pushups_outlier)\n\n\n[1] 55\n\n\nRange jumped from 20 to 55 due to a single outlier!"
  },
  {
    "objectID": "lesson_04.html#the-interquartile-range-iqr",
    "href": "lesson_04.html#the-interquartile-range-iqr",
    "title": "Lesson 4: Measures of Variability",
    "section": "4 The Interquartile Range (IQR)",
    "text": "4 The Interquartile Range (IQR)\n\n\n\n\n\n\nDefinition: Interquartile Range\n\n\n\nThe interquartile range (IQR) is the range of the middle 50% of the data:\n\\[\\text{IQR} = Q_3 - Q_1\\]\nwhere \\(Q_1\\) is the first quartile (25th percentile) and \\(Q_3\\) is the third quartile (75th percentile).\n\n\n\n4.1 Why IQR is Better Than Range\n\nResistant to outliers: Uses middle 50%, ignoring extreme values\nFocuses on typical values: Describes spread of the central bulk of data\nUseful with skewed data: Not affected by long tails\n\n\n\n4.2 Example 4.2: Calculating IQR\nRuck march times (minutes): 95, 100, 105, 110, 115, 120, 125, 130, 150\n\n\nCode\nruck_times &lt;- c(95, 100, 105, 110, 115, 120, 125, 130, 150)\n\n# Find quartiles\nQ1 &lt;- quantile(ruck_times, 0.25)\nQ3 &lt;- quantile(ruck_times, 0.75)\n\n# Calculate IQR\nIQR_val &lt;- Q3 - Q1\n\ncat(\"Q1 =\", Q1, \"\\n\")\n\n\nQ1 = 105 \n\n\nCode\ncat(\"Q3 =\", Q3, \"\\n\")\n\n\nQ3 = 125 \n\n\nCode\ncat(\"IQR =\", IQR_val, \"\\n\")\n\n\nIQR = 20 \n\n\nInterpretation: The middle 50% of ruck times span 22.5 minutes.\nNote: The outlier (150) doesn’t affect IQR!\n\n\n4.3 IQR in R\n\n\nCode\n# R has a built-in IQR function\nIQR(ruck_times)\n\n\n[1] 20\n\n\n\n\n4.4 The Five-Number Summary\nThe IQR is part of the five-number summary:\n\nMinimum\n\\(Q_1\\) (25th percentile)\nMedian (50th percentile)\n\\(Q_3\\) (75th percentile)\nMaximum\n\n\n\nCode\nsummary(ruck_times)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   95.0   105.0   115.0   116.7   125.0   150.0 \n\n\nThis is exactly what a boxplot displays visually!\n\n\nCode\nggplot(tibble(times = ruck_times), aes(y = times)) +\n  geom_boxplot(fill = \"steelblue\", width = 0.3) +\n  labs(title = \"Boxplot of Ruck March Times\",\n       y = \"Time (minutes)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())"
  },
  {
    "objectID": "lesson_04.html#variance-and-standard-deviation",
    "href": "lesson_04.html#variance-and-standard-deviation",
    "title": "Lesson 4: Measures of Variability",
    "section": "5 Variance and Standard Deviation",
    "text": "5 Variance and Standard Deviation\nRange and IQR tell us about spread, but they don’t use all the data. Variance and standard deviation consider every observation’s distance from the mean.\n\n5.1 The Sample Variance\n\n\n\n\n\n\nDefinition: Sample Variance\n\n\n\nThe sample variance \\(s^2\\) measures the average squared deviation from the mean:\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\]\nWhy \\(n-1\\) instead of \\(n\\)? This is Bessel’s correction, which makes \\(s^2\\) an unbiased estimator of the population variance \\(\\sigma^2\\).\n\n\n\n\n5.2 Step-by-Step Calculation\n\n\n5.3 Example 4.3: Computing Variance by Hand\nFive soldiers’ ACFT scores: 450, 470, 480, 490, 510\nStep 1: Calculate the mean \\[\\bar{x} = \\frac{450 + 470 + 480 + 490 + 510}{5} = \\frac{2400}{5} = 480\\]\nStep 2: Calculate each deviation from the mean\n\n\n\n\n\nScore (x)\nDeviation (x - x̄\n| Squared Deviation (x - x̄\n\n\n\n\n450\n-30\n900\n\n\n470\n-10\n100\n\n\n480\n0\n0\n\n\n490\n10\n100\n\n\n510\n30\n900\n\n\n\n\n\nStep 3: Sum the squared deviations \\[\\sum_{i=1}^5 (x_i - \\bar{x})^2 = 900 + 100 + 0 + 100 + 900 = 2000\\]\nStep 4: Divide by \\(n-1\\) \\[s^2 = \\frac{2000}{5-1} = \\frac{2000}{4} = 500\\]\n\n\nCode\n# Verify with R\nvar(scores)\n\n\n[1] 500\n\n\n\n\n5.4 The Sample Standard Deviation\nThe variance has units of “squared scores” (e.g., points²), which is hard to interpret. The standard deviation brings us back to original units.\n\n\n\n\n\n\nDefinition: Sample Standard Deviation\n\n\n\nThe sample standard deviation \\(s\\) is the square root of the variance:\n\\[s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2}\\]\nUnits: Same as the original data\n\n\n\n\nCode\n# Standard deviation of ACFT scores\nsd(scores)\n\n\n[1] 22.36068\n\n\nInterpretation: On average, ACFT scores deviate from the mean by about 22.4 points.\n\n\n5.5 Properties of Variance and Standard Deviation\n\nAlways non-negative: \\(s^2 \\geq 0\\) and \\(s \\geq 0\\)\nZero only when all values are identical: If \\(s = 0\\), every observation equals \\(\\bar{x}\\)\nSensitive to outliers: Squaring deviations amplifies the effect of extreme values\nSame units as data (for \\(s\\)): Makes interpretation easier\nIncreases with spread: Larger \\(s\\) means more variability"
  },
  {
    "objectID": "lesson_04.html#comparing-variability",
    "href": "lesson_04.html#comparing-variability",
    "title": "Lesson 4: Measures of Variability",
    "section": "6 Comparing Variability",
    "text": "6 Comparing Variability\nLet’s compare two datasets with the same mean but different variability:\n\n\nCode\n# Two platoons, both with mean = 480\nset.seed(123)\nplatoon_A &lt;- rnorm(30, mean = 480, sd = 15)  # Low variability\nplatoon_B &lt;- rnorm(30, mean = 480, sd = 50)  # High variability\n\n# Summary statistics\ncomparison &lt;- tibble(\n  Platoon = c(\"A (consistent)\", \"B (variable)\"),\n  Mean = c(mean(platoon_A), mean(platoon_B)),\n  SD = c(sd(platoon_A), sd(platoon_B)),\n  IQR = c(IQR(platoon_A), IQR(platoon_B)),\n  Range = c(max(platoon_A) - min(platoon_A), max(platoon_B) - min(platoon_B))\n) %&gt;%\n  mutate(across(where(is.numeric), ~round(., 1)))\n\ncomparison %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nPlatoon\nMean\nSD\nIQR\nRange\n\n\n\n\nA (consistent)\n479.3\n14.7\n17.4\n56.3\n\n\nB (variable)\n488.9\n41.8\n53.0\n185.9\n\n\n\n\n\nVisualization:\n\n\nCode\ncombined_data &lt;- tibble(\n  Score = c(platoon_A, platoon_B),\n  Platoon = rep(c(\"A (consistent)\", \"B (variable)\"), each = 30)\n)\n\nggplot(combined_data, aes(x = Score, fill = Platoon)) +\n  geom_histogram(bins = 15, alpha = 0.6, position = \"identity\") +\n  geom_vline(xintercept = 480, linetype = \"dashed\", linewidth = 1) +\n  scale_fill_manual(values = c(\"steelblue\", \"coral\")) +\n  labs(title = \"Two Platoons with Same Mean, Different Variability\",\n       subtitle = \"Both centered at 480, but very different spreads\",\n       x = \"ACFT Score\", y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "lesson_04.html#the-empirical-rule-68-95-99.7-rule",
    "href": "lesson_04.html#the-empirical-rule-68-95-99.7-rule",
    "title": "Lesson 4: Measures of Variability",
    "section": "7 The Empirical Rule (68-95-99.7 Rule)",
    "text": "7 The Empirical Rule (68-95-99.7 Rule)\nFor bell-shaped (normal) distributions, standard deviation has special meaning:\n\n\n\n\n\n\nThe Empirical Rule\n\n\n\nFor approximately normal distributions:\n\nAbout 68% of data falls within 1 standard deviation of the mean: \\([\\bar{x} - s, \\bar{x} + s]\\)\nAbout 95% of data falls within 2 standard deviations of the mean: \\([\\bar{x} - 2s, \\bar{x} + 2s]\\)\nAbout 99.7% of data falls within 3 standard deviations of the mean: \\([\\bar{x} - 3s, \\bar{x} + 3s]\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.1 Example 4.4: Applying the Empirical Rule\nACFT scores are approximately normal with \\(\\bar{x} = 480\\) and \\(s = 50\\).\nQuestion: What range contains the middle 95% of scores?\nSolution: Use the Empirical Rule with \\(k = 2\\):\n\\[[\\bar{x} - 2s, \\bar{x} + 2s] = [480 - 2(50), 480 + 2(50)] = [380, 580]\\]\nAbout 95% of ACFT scores fall between 380 and 580."
  },
  {
    "objectID": "lesson_04.html#detecting-outliers-with-iqr",
    "href": "lesson_04.html#detecting-outliers-with-iqr",
    "title": "Lesson 4: Measures of Variability",
    "section": "8 Detecting Outliers with IQR",
    "text": "8 Detecting Outliers with IQR\nThe IQR provides a formal method for identifying outliers:\n\n\n\n\n\n\nOutlier Detection Rule\n\n\n\nAn observation is considered an outlier if:\n\\[x &lt; Q_1 - 1.5 \\times \\text{IQR} \\quad \\text{or} \\quad x &gt; Q_3 + 1.5 \\times \\text{IQR}\\]\nThese are called the lower and upper fences.\n\n\n\n8.1 Example 4.5: Identifying Outliers\nRuck march times: 95, 100, 105, 110, 115, 120, 125, 130, 150\n\n\nCode\nruck_times &lt;- c(95, 100, 105, 110, 115, 120, 125, 130, 150)\n\nQ1 &lt;- quantile(ruck_times, 0.25)\nQ3 &lt;- quantile(ruck_times, 0.75)\nIQR_val &lt;- Q3 - Q1\n\nlower_fence &lt;- Q1 - 1.5 * IQR_val\nupper_fence &lt;- Q3 + 1.5 * IQR_val\n\ncat(\"Q1 =\", Q1, \"\\n\")\n\n\nQ1 = 105 \n\n\nCode\ncat(\"Q3 =\", Q3, \"\\n\")\n\n\nQ3 = 125 \n\n\nCode\ncat(\"IQR =\", IQR_val, \"\\n\")\n\n\nIQR = 20 \n\n\nCode\ncat(\"Lower fence =\", lower_fence, \"\\n\")\n\n\nLower fence = 75 \n\n\nCode\ncat(\"Upper fence =\", upper_fence, \"\\n\")\n\n\nUpper fence = 155 \n\n\nCode\n# Identify outliers\noutliers &lt;- ruck_times[ruck_times &lt; lower_fence | ruck_times &gt; upper_fence]\ncat(\"Outliers:\", outliers, \"\\n\")\n\n\nOutliers:  \n\n\nThe value 150 exceeds the upper fence (144.375), so it’s flagged as an outlier."
  },
  {
    "objectID": "lesson_04.html#coefficient-of-variation",
    "href": "lesson_04.html#coefficient-of-variation",
    "title": "Lesson 4: Measures of Variability",
    "section": "9 Coefficient of Variation",
    "text": "9 Coefficient of Variation\nSometimes we want to compare variability across datasets with different units or scales. The coefficient of variation provides a unitless measure.\n\n\n\n\n\n\nDefinition: Coefficient of Variation\n\n\n\nThe coefficient of variation (CV) expresses standard deviation as a percentage of the mean:\n\\[\\text{CV} = \\frac{s}{\\bar{x}} \\times 100\\%\\]\nWhen to use: Comparing variability of datasets with different units or magnitudes\n\n\n\n9.1 Example 4.6: Coefficient of Variation\nCompare variability of:\n\n2-mile run times: \\(\\bar{x} = 14\\) min, \\(s = 1.5\\) min\n12-mile ruck times: \\(\\bar{x} = 150\\) min, \\(s = 15\\) min\n\n\n\nCode\n# 2-mile run\nCV_run &lt;- (1.5 / 14) * 100\n\n# 12-mile ruck\nCV_ruck &lt;- (15 / 150) * 100\n\ncat(\"2-mile run CV:\", round(CV_run, 1), \"%\\n\")\n\n\n2-mile run CV: 10.7 %\n\n\nCode\ncat(\"12-mile ruck CV:\", round(CV_ruck, 1), \"%\\n\")\n\n\n12-mile ruck CV: 10 %\n\n\nBoth have 10% coefficient of variation, indicating similar relative variability despite different scales."
  },
  {
    "objectID": "lesson_04.html#comprehensive-example-analyzing-apft-data",
    "href": "lesson_04.html#comprehensive-example-analyzing-apft-data",
    "title": "Lesson 4: Measures of Variability",
    "section": "10 Comprehensive Example: Analyzing APFT Data",
    "text": "10 Comprehensive Example: Analyzing APFT Data\n\n\nCode\n# Simulate APFT component scores for 50 soldiers\nset.seed(206)\napft_data &lt;- tibble(\n  soldier_id = 1:50,\n  pushups = round(rnorm(50, mean = 70, sd = 10)),\n  situps = round(rnorm(50, mean = 75, sd = 8)),\n  run_time = round(rnorm(50, mean = 13.5, sd = 1.2), 1)\n)\n\n# Calculate measures of spread for each event\nspread_summary &lt;- tibble(\n  Event = c(\"Push-ups\", \"Sit-ups\", \"Run Time (min)\"),\n  Mean = c(mean(apft_data$pushups), mean(apft_data$situps), mean(apft_data$run_time)),\n  SD = c(sd(apft_data$pushups), sd(apft_data$situps), sd(apft_data$run_time)),\n  Variance = c(var(apft_data$pushups), var(apft_data$situps), var(apft_data$run_time)),\n  IQR = c(IQR(apft_data$pushups), IQR(apft_data$situps), IQR(apft_data$run_time)),\n  Range = c(max(apft_data$pushups) - min(apft_data$pushups),\n            max(apft_data$situps) - min(apft_data$situps),\n            max(apft_data$run_time) - min(apft_data$run_time)),\n  CV = c(sd(apft_data$pushups) / mean(apft_data$pushups) * 100,\n         sd(apft_data$situps) / mean(apft_data$situps) * 100,\n         sd(apft_data$run_time) / mean(apft_data$run_time) * 100)\n) %&gt;%\n  mutate(across(where(is.numeric), ~round(., 2)))\n\nspread_summary %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nEvent\nMean\nSD\nVariance\nIQR\nRange\nCV\n\n\n\n\nPush-ups\n71.64\n9.17\n84.03\n11.75\n43.0\n12.80\n\n\nSit-ups\n75.06\n8.74\n76.30\n12.75\n41.0\n11.64\n\n\nRun Time (min)\n13.58\n1.01\n1.02\n1.40\n4.2\n7.45\n\n\n\n\n\nComparative boxplots:\n\n\nCode\napft_long &lt;- apft_data %&gt;%\n  pivot_longer(cols = c(pushups, situps), names_to = \"Event\", values_to = \"Reps\")\n\nggplot(apft_long, aes(x = Event, y = Reps, fill = Event)) +\n  geom_boxplot(alpha = 0.7) +\n  scale_fill_manual(values = c(\"steelblue\", \"coral\")) +\n  labs(title = \"Comparison of Push-up and Sit-up Variability\",\n       y = \"Repetitions\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "lesson_04.html#practice-problems",
    "href": "lesson_04.html#practice-problems",
    "title": "Lesson 4: Measures of Variability",
    "section": "11 Practice Problems",
    "text": "11 Practice Problems\n\n11.1 Practice 4.1\nCalculate the range, IQR, variance, and standard deviation for these datasets:\na) Platoon ACFT scores: 420, 450, 470, 480, 490, 510, 540\nb) Ruck march times (min): 100, 105, 110, 115, 120, 125, 180\n\n\n11.2 Practice 4.2\nTwo companies have the following statistics for ruck march times:\n\nCompany A: \\(\\bar{x} = 120\\) min, \\(s = 10\\) min\nCompany B: \\(\\bar{x} = 115\\) min, \\(s = 25\\) min\n\n\nWhich company has more consistent performance?\nCalculate the coefficient of variation for each company\nIf the Empirical Rule applies, what range contains the middle 95% of times for Company A?\n\n\n\n11.3 Practice 4.3\nDataset: 10, 12, 14, 15, 16, 18, 20, 22, 45\n\nCalculate \\(Q_1\\), \\(Q_3\\), and IQR\nDetermine the lower and upper fences\nIdentify any outliers\nHow would removing the outlier affect the mean and standard deviation?\n\n\n\n11.4 Practice 4.4\nTrue or False (explain your reasoning):\n\nA dataset with range = 0 must have \\(s = 0\\)\nA dataset with \\(s = 0\\) must have range = 0\nIf all values in a dataset are doubled, the standard deviation doubles\nIf 10 is added to every value, the standard deviation increases by 10\nIQR is always less than or equal to the range"
  },
  {
    "objectID": "lesson_04.html#summary",
    "href": "lesson_04.html#summary",
    "title": "Lesson 4: Measures of Variability",
    "section": "12 Summary",
    "text": "12 Summary\nKey Takeaways:\n\nRange = max - min\n\nSimple but sensitive to outliers\n\nInterquartile Range (IQR) = \\(Q_3 - Q_1\\)\n\nResistant to outliers\nDescribes middle 50% of data\n\nVariance \\(s^2 = \\frac{1}{n-1}\\sum(x_i - \\bar{x})^2\\)\n\nAverage squared deviation from mean\nUnits are squared\n\nStandard Deviation \\(s = \\sqrt{s^2}\\)\n\nSquare root of variance\nSame units as data\nMost commonly used measure of spread\n\nEmpirical Rule (for normal distributions):\n\n68% within 1 SD\n95% within 2 SD\n99.7% within 3 SD\n\nOutlier detection: \\(x &lt; Q_1 - 1.5 \\times \\text{IQR}\\) or \\(x &gt; Q_3 + 1.5 \\times \\text{IQR}\\)\n\nNext: In Lesson 5, we’ll put everything together in an Exploratory Data Analysis Lab using real datasets!"
  },
  {
    "objectID": "lesson_04.html#additional-resources",
    "href": "lesson_04.html#additional-resources",
    "title": "Lesson 4: Measures of Variability",
    "section": "13 Additional Resources",
    "text": "13 Additional Resources\n\nDevore Section 1.4\nUnderstanding Standard Deviation\nWebAssign homework on measures of variability"
  },
  {
    "objectID": "sections/ma206x_sectioning.html",
    "href": "sections/ma206x_sectioning.html",
    "title": "ma206x_sectioning",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(patchwork)\n\nstem_codes &lt;- c(\n  # Engineering\n  \"CEN2\",\"CES2\",\"CVN3\",\"CNG2\",\"EVE2\",\"EES2\",\"EEN1\",\"MEN3\",\"MES3\",\n  \"ORE2\",\"ORS1\",\"ENM1\",\"SEN2\",\"NEN1\",\"NES1\",\"AEN0\",\n  # Sciences & Math\n  \"BIO0\",\"CHM2\",\"PHY1\",\"PHS0\",\"SSC0\",\"MSC1\",\"MST1\",\"ASD0\",\n  # Computer/Cyber\n  \"CSC1\",\"CYS1\",\"CYO0\",\"EIT2\",\n  # Environmental/Geo\n  \"ESC3\",\"GEO1\",\"GSC0\"\n)\n\n\ndf = read_csv(\"ma206x.csv\") %&gt;%\n  mutate(corps_squad = recode(corps_squad,\n                              \"null\" = \"No\",\n                              \"YES\"  = \"Yes\"),\n         stem_flag = if_else(maj %in% stem_codes, 1, 0),\n         jedi = if_else(ma153 != \"null\", 1, 0))\n\n\n\n\nCode\ndf %&gt;% count(stem_flag)\n\n\n# A tibble: 2 × 2\n  stem_flag     n\n      &lt;dbl&gt; &lt;int&gt;\n1         0   221\n2         1   235\n\n\n\n\nCode\nset.seed(123)\n\ndf_sample &lt;- df %&gt;%\n  mutate(bin = ntile(apsc, 10),\n         w = if_else(corps_squad == \"Yes\", 0.5, 1)) %&gt;%\n  group_by(bin) %&gt;%\n  slice_sample(prop = 145/456, weight_by = w) %&gt;%\n  ungroup()\n\nmean_df &lt;- mean(df$apsc)\nsd_df   &lt;- sd(df$apsc)\nmean_sample &lt;- mean(df_sample$apsc)\nsd_sample   &lt;- sd(df_sample$apsc)\ncorp_df &lt;- mean(df$corps_squad == \"Yes\") * 100\ncorp_samp &lt;- mean(df_sample$corps_squad == \"Yes\") * 100\njedi_df &lt;- mean(df$jedi) * 100\njedi_samp &lt;- mean(df_sample$jedi) * 100\nstem_df &lt;- mean(df$stem_flag) * 100\nstem_samp &lt;- mean(df_sample$stem_flag) * 100\n\na &lt;- df %&gt;% ggplot(aes(apsc, fill=corps_squad)) + geom_histogram(bins=35) + theme_bw() + \n  labs(title=\"AY26-2 MA206 Cohort\", x=\"APSC\", y=\"Count\", fill = \"Corps Squad\") +\n  annotate(\"text\", x = 2.4, y = 26.5,\n           label = paste0(\"Mean APSC = \", round(mean_df, 2), \n                          \"\\nSD APSC = \", round(sd_df, 2),\n                          \"\\n Corps Squad = \", round(corp_df),'%',\n                          \"\\n Jedi = \", round(jedi_df),'%',\n                          \"\\n STEM = \", round(stem_df),'%'),\n           hjust = 1) +\n  coord_cartesian(xlim = c(1.5, 4.0))\nb &lt;- df_sample %&gt;% ggplot(aes(apsc, fill = corps_squad)) + geom_histogram(bins = 35) + theme_bw() + labs(title=\"Prospective MA206X w/Alternates\", x=\"APSC\", y=\"Count\", fill = \"Corps Squad\") +\n  annotate(\"text\", x = 2.5, y = 7.5,\n           label = paste0(\"Mean APSC = \", round(mean_sample, 2), \n                          \"\\nSD APSC = \", round(sd_sample, 2),\n                          \"\\n Corps Squad = \", round(corp_samp),'%',\n                          \"\\n Jedi = \", round(jedi_samp),'%',\n                          \"\\n STEM = \", round(stem_samp),'%'),\n           hjust = 1) +\n  coord_cartesian(xlim = c(1.5, 4.0))\n\nc = a + b + plot_layout(guides = \"collect\")\n\nd &lt;- df %&gt;% ggplot(aes(apsc, fill = if_else(jedi == 1, \"Yes\", \"No\"))) + geom_histogram(bins=35) + theme_bw() + \n  labs(title=\"AY26-2 MA206 Cohort\", x=\"APSC\", y=\"Count\", fill = \"Corps Squad\") +\n  annotate(\"text\", x = 2.4, y = 26.5,\n           label = paste0(\"Mean APSC = \", round(mean_df, 2), \n                          \"\\nSD APSC = \", round(sd_df, 2),\n                          \"\\n Corps Squad = \", round(corp_df),'%',\n                          \"\\n Jedi = \", round(jedi_df),'%',\n                          \"\\n STEM = \", round(stem_df),'%'),\n           hjust = 1) +\n  coord_cartesian(xlim = c(1.5, 4.0))\ne &lt;- df_sample %&gt;% ggplot(aes(apsc, fill = if_else(jedi == 1, \"Yes\", \"No\"))) + geom_histogram(bins = 35) + theme_bw() + labs(title=\"Prospective MA206X w/Alternates\", x=\"APSC\", y=\"Count\", fill = \"Jedi\") +\n  annotate(\"text\", x = 2.5, y = 7.5,\n           label = paste0(\"Mean APSC = \", round(mean_sample, 2), \n                          \"\\nSD APSC = \", round(sd_sample, 2),\n                          \"\\n Corps Squad = \", round(corp_samp),'%',\n                          \"\\n Jedi = \", round(jedi_samp),'%',\n                          \"\\n STEM = \", round(stem_samp),'%'),\n           hjust = 1) +\n  coord_cartesian(xlim = c(1.5, 4.0))\n\nf &lt;- d + e + plot_layout(guides = \"collect\")\nf\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Weighted sample (down-weight Corps Squad)\ndf_sample &lt;- df %&gt;%\n  mutate(bin = ntile(apsc, 10)) %&gt;%\n  group_by(bin) %&gt;%\n  slice_sample(prop = 145/456) %&gt;%\n  ungroup()\n\n# Summary stats\nmean_df &lt;- mean(df$apsc)\nsd_df   &lt;- sd(df$apsc)\nmean_sample &lt;- mean(df_sample$apsc)\nsd_sample   &lt;- sd(df_sample$apsc)\n\ncorp_df &lt;- mean(df$corps_squad == \"Yes\") * 100\ncorp_samp &lt;- mean(df_sample$corps_squad == \"Yes\") * 100\njedi_df &lt;- mean(df$jedi) * 100\njedi_samp &lt;- mean(df_sample$jedi) * 100\nstem_df &lt;- mean(df$stem_flag) * 100\nstem_samp &lt;- mean(df_sample$stem_flag) * 100\n\n# -------- 1st row: Corps Squad --------\na &lt;- df %&gt;% \n  ggplot(aes(apsc, fill = corps_squad)) +\n  geom_histogram(bins=35) + theme_bw() +\n  labs(title=\"AY26-2 MA206 Cohort\", x=\"APSC\", y=\"Count\", fill = \"Corps Squad\") +\n  annotate(\"text\", x = 2.4, y = 25,\n           label = paste0(\"Mean APSC = \", round(mean_df, 2), \n                          \"\\nSD APSC = \", round(sd_df, 2),\n                          \"\\n Corps Squad = \", round(corp_df),'%',\n                          \"\\n Jedi = \", round(jedi_df),'%',\n                          \"\\n STEM = \", round(stem_df),'%'),\n           hjust = 1) +\n  coord_cartesian(xlim = c(1.5, 4.0)) + theme(legend.position = \"none\")\n\nb &lt;- df_sample %&gt;% \n  ggplot(aes(apsc, fill = corps_squad)) +\n  geom_histogram(bins = 35) + theme_bw() +\n  labs(title=\"Prospective MA206X w/Alternates\", x=\"APSC\", y=\"Count\", fill = \"Corps Squad\") +\n  annotate(\"text\", x = 2.5, y = 8,\n           label = paste0(\"Mean APSC = \", round(mean_sample, 2), \n                          \"\\nSD APSC = \", round(sd_sample, 2),\n                          \"\\n Corps Squad = \", round(corp_samp),'%',\n                          \"\\n Jedi = \", round(jedi_samp),'%',\n                          \"\\n STEM = \", round(stem_samp),'%'),\n           hjust = 1) +\n  coord_cartesian(xlim = c(1.5, 4.0))\n\n# -------- 2nd row: Jedi --------\nd &lt;- df %&gt;% \n  ggplot(aes(apsc, fill = if_else(jedi == 1, \"Yes\", \"No\"))) +\n  geom_histogram(bins=35) + theme_bw() +\n  labs(title=\"AY26-2 MA206 Cohort\", x=\"APSC\", y=\"Count\", fill = \"Jedi\") +\n  coord_cartesian(xlim = c(1.5, 4.0))+theme(legend.position = \"none\")\n\ne &lt;- df_sample %&gt;% \n  ggplot(aes(apsc, fill = if_else(jedi == 1, \"Yes\", \"No\"))) +\n  geom_histogram(bins = 35) + theme_bw() +\n  labs(title=\"Prospective MA206X w/Alternates\", x=\"APSC\", y=\"Count\", fill = \"Jedi\") +\n  coord_cartesian(xlim = c(1.5, 4.0))\n\n# -------- 3rd row: STEM --------\ng &lt;- df %&gt;% \n  ggplot(aes(apsc, fill = if_else(stem_flag == 1, \"Yes\", \"No\"))) +\n  geom_histogram(bins=35) + theme_bw() +\n  labs(title=\"AY26-2 MA206 Cohort\", x=\"APSC\", y=\"Count\", fill = \"STEM\") +\n  coord_cartesian(xlim = c(1.5, 4.0))+theme(legend.position = \"none\")\n\nh &lt;- df_sample %&gt;% \n  ggplot(aes(apsc, fill = if_else(stem_flag == 1, \"Yes\", \"No\"))) +\n  geom_histogram(bins = 35) + theme_bw() +\n  labs(title=\"Prospective MA206X w/Alternates\", x=\"APSC\", y=\"Count\", fill = \"STEM\") +\n  coord_cartesian(xlim = c(1.5, 4.0))\n\n# -------- Combine into 3x2 grid --------\nfinal_plot &lt;- (a + b) / (d + e) / (g + h)\n\nfinal_plot +\n  theme(\n    panel.border = element_rect(color = \"black\", fill = NA, linewidth = 0.25),\n    panel.spacing = unit(0.2, \"lines\"),\n    panel.grid.major = element_line(color = \"grey85\", linewidth = 0.3),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(123)\n\n# Weighted sample (down-weight Corps Squad)\ndf_sample &lt;- df %&gt;%\n  mutate(bin = ntile(apsc, 10),\n         w = if_else(corps_squad == \"Yes\", 0.5, 1)) %&gt;%\n  group_by(bin) %&gt;%\n  slice_sample(prop = 140/456, weight_by = w) %&gt;%\n  ungroup()\n\n# Add cohort labels\ndf_long &lt;- df %&gt;%\n  mutate(cohort = \"AY26-2 MA206 Cohort\") %&gt;%\n  bind_rows(df_sample %&gt;% mutate(cohort = \"Prospective MA206X w/Alternates\"))\n\n# Pivot to long format for Corps Squad, Jedi, STEM\ndf_long &lt;- df_long %&gt;%\n  mutate(\n    Jedi = if_else(jedi == 1, \"Yes\", \"No\"),\n    STEM = if_else(stem_flag == 1, \"Yes\", \"No\")\n  ) %&gt;%\n  pivot_longer(\n    cols = c(corps_squad, Jedi, STEM),\n    names_to = \"Group\",\n    values_to = \"Category\"\n  )\n\n# Plot\nfinal_plot &lt;- ggplot(df_long, aes(x = apsc, fill = Category)) +\n  geom_histogram(bins = 35) +\n  theme_bw() +\n  coord_cartesian(xlim = c(1.5, 4.0)) +\n  labs(x = \"APSC\", y = \"Count\") +\n  facet_grid(Group ~ cohort, scales = \"free_y\") +\n  theme(legend.position = \"bottom\")\n\nfinal_plot\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_sample\n\n\n# A tibble: 136 × 26\n   cnum     last  first email sex      yr co    maj   maj2  min    apsc apsc_maj\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 C360497… GELD… FRAN… fran… M      2028 F4    ORE2  null  null   2.19     0   \n 2 C772022… REBO… SABR… sabr… F      2028 A2    ASD0  null  null   2.12     0   \n 3 C317408… MERC… MASON maso… M      2028 C3    HUS3T null  null   2.24     0   \n 4 C383737… SCHM… WALT… walt… M      2028 G4    ENM1  null  null   2.35     0   \n 5 C185030… BELL  JAXS… jaxs… M      2028 D4    SDS1  null  null   2.15     3.33\n 6 C575418… TORR… ISIS  isis… F      2026 E3    MEN3  null  null   2.22     2.11\n 7 C480196… PECK… NATH… nath… M      2028 H1    SSC0  null  null   2.29     0   \n 8 C098567… MOORE TAYT… tayt… F      2028 H1    ENL2  null  null   2.28     0   \n 9 C897564… MCGL… ALEK  alek… M      2028 E2    BEO0  null  null   2.18     0   \n10 C648214… DE S… JOSE… jose… F      2028 E1    PIF0  null  null   2.32     0   \n# ℹ 126 more rows\n# ℹ 14 more variables: cqpa &lt;dbl&gt;, en_seq &lt;chr&gt;, corps_squad &lt;chr&gt;,\n#   activity &lt;chr&gt;, ma103 &lt;chr&gt;, ma104 &lt;chr&gt;, ma153 &lt;chr&gt;, ma205 &lt;chr&gt;,\n#   cy105 &lt;chr&gt;, cy305 &lt;chr&gt;, stem_flag &lt;dbl&gt;, jedi &lt;dbl&gt;, bin &lt;int&gt;, w &lt;dbl&gt;\n\n\n\n\nCode\ndf\n\n\n# A tibble: 456 × 24\n   cnum     last  first email sex      yr co    maj   maj2  min    apsc apsc_maj\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 C713523… ABDA… IAN   ian.… M      2028 H1    ENM1  null  null   3.72     0   \n 2 C207421… ABOUD AIDAN aida… M      2028 I4    ENL2  null  null   4.28     0   \n 3 C545211… ACEV… ANGEL ange… M      2028 A3    CEN2  null  null   2.81     0   \n 4 C613922… ACUN… OLIV… oliv… M      2028 D1    ENM1  null  null   2.56     0   \n 5 C965985… AEMI… HANN… hann… F      2028 G2    ORE2  null  RSE1N  3.61     0   \n 6 C467024… AGRO  SALV… salv… M      2028 C2    ENM1  null  null   2.78     0   \n 7 C277387… AIRD  ZAND… zand… M      2028 E3    ENM1  null  null   2.46     0   \n 8 C717140… AMBR… OMAR  omar… M      2028 G3    CVN3  null  null   2.83     0   \n 9 C722892… ANDE… RILEY rile… F      2028 E3    ESC3  null  null   3.33     3.33\n10 C784889… ANDE… BRAD… brad… M      2028 I3    MGN0  null  null   2.52     0   \n# ℹ 446 more rows\n# ℹ 12 more variables: cqpa &lt;dbl&gt;, en_seq &lt;chr&gt;, corps_squad &lt;chr&gt;,\n#   activity &lt;chr&gt;, ma103 &lt;chr&gt;, ma104 &lt;chr&gt;, ma153 &lt;chr&gt;, ma205 &lt;chr&gt;,\n#   cy105 &lt;chr&gt;, cy305 &lt;chr&gt;, stem_flag &lt;dbl&gt;, jedi &lt;dbl&gt;\n\n\n\n\nCode\nmodel &lt;- lm(apsc ~ maj, data=df)\n\n\n\n\nCode\nanova(model)\n\n\nAnalysis of Variance Table\n\nResponse: apsc\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nmaj        57  34.687 0.60855  2.3493 9.118e-07 ***\nResiduals 398 103.097 0.25904                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\noutput &lt;- TukeyHSD(aov(model))\n\n\n\n\nCode\noutput$maj[output$maj[,4] &lt; 0.05]\n\n\n [1] -8.403909e-01 -1.258936e+00 -1.167240e+00 -6.887407e-01 -9.522692e-01\n [6] -7.403106e-01 -7.599189e-01 -8.198552e-01 -8.891373e-01 -8.223456e-01\n[11] -1.512445e+00 -2.287713e+00 -1.935231e+00 -1.365769e+00 -1.694882e+00\n[16] -1.395583e+00 -1.396637e+00 -1.587847e+00 -1.675053e+00 -1.548392e+00\n[21] -1.683366e-01 -2.301587e-01 -3.992485e-01 -1.171246e-02 -2.096569e-01\n[26] -8.503797e-02 -1.232006e-01 -5.186385e-02 -1.032219e-01 -9.629922e-02\n[31]  6.864315e-04  1.152574e-03  1.978807e-06  3.892209e-02  3.645491e-04\n[36]  6.396581e-03  2.037771e-03  1.801832e-02  6.225469e-03  6.087061e-03\n\n\n\n\nCode\nn = 5\np = 0.5\n\npbinom(3, n, p)\n\n\n[1] 0.8125\n\n\n\n\nCode\n1 - pbinom(3,n,p)\n\n\n[1] 0.1875\n\n\n\n\nCode\nhelp(pbinom)"
  }
]